{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bn_face_VGG16_random_datasplit_seed_set_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfwg9P9tpQOMkSLms9Z6nM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amita-kapoor/FaceRecognition/blob/master/Source/HNN/Bn_face_VGG16_random_datasplit_seed_set_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwK8V76GUAwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Face recognition for home surveillance using transfer learning: Base model is VGG16 model trained on imagenet dataset. \n",
        "# training and validation data split randomly\n",
        "# seed_set: random, numoy, os, keras initializer, tf\n",
        "# calculated train, test, validation accuracy\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kluWnbZ336fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#experiment = Experiment(project_name=\"Classification model\")\n",
        "#experiment.log_other(\"random seed\", 10)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGllT-HaPNpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://drive.google.com/file/d/16Cpr_K8wflV1SiMs3yH3KPCpyG-NwFsB/view?usp=sharing\n",
        "# random split\n",
        "# https://drive.google.com/file/d/1fjBB65hrc7iJN4hv8_djFtWGtwOxAaKz/view?usp=sharing\n",
        "\n",
        "# https://drive.google.com/file/d/1pg3fIhD_jPadsDIwJ4Lsjk5LCydqp1xb/view?usp=sharing"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiO7-bNpzHAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "671876f7-b9c2-4d4c-8681-1962524713e3"
      },
      "source": [
        "!gdown --id 1pg3fIhD_jPadsDIwJ4Lsjk5LCydqp1xb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pg3fIhD_jPadsDIwJ4Lsjk5LCydqp1xb\n",
            "To: /content/faceImages3.zip\n",
            "8.80MB [00:00, 21.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqLJy7NvzbBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir \"faceImages\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0_-cVRszn91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c36da50-3ff6-42ac-fb3d-cbe13695f6e2"
      },
      "source": [
        "type('faceImages3.zip')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M2ZZQyAzrw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c3cc99d-84ad-40d8-cf2c-ca4fff880a12"
      },
      "source": [
        "!unzip \"/content/faceImages3.zip\" -d \"/content/faceImages/\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/faceImages3.zip\n",
            "   creating: /content/faceImages/faceImages3/\n",
            "   creating: /content/faceImages/faceImages3/data/\n",
            "   creating: /content/faceImages/faceImages3/data/Chirag/\n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_298.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_299.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_300.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_301.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_305.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_306.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_317.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Chirag/face_462.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Gunjan/\n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_154.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_175.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_229.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_349.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_461.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_465.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_469.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_473.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_480.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_486.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_504.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_507.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_509.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_510.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_512.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_519.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Gunjan/face_76.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Jyoti/\n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_120.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_453.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_457.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_474.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_475.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_477.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_479.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_485.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_487.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Jyoti/face_84.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Kiran/\n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_350.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_357.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_358.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_360.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_363.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_365.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_366.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_373.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_375.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_379.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_380.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_384.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_385.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_400.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_404.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_405.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_407.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_409.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_411.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_413.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_419.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_423.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_428.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_434.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_478.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Kiran/face_86.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Meghna/\n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_19.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_451.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_467.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_469.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_470.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_501.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_502.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_503.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_504.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_550.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_553.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_554.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_557.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_559.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_561.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_563.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_564.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_567.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_571.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_572.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_579.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_580.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_581.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_583.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_587.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Meghna/face_591.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Mohit/\n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_356.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_359.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_362.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_367.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_372.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_378.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_403.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_406.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Mohit/face_425.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Neha/\n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_0.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_100.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_102.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_103.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_11.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_110.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_113.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_114.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_116.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_118.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_122.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_124.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_128.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_133.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_134.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_135.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_137.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_138.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_141.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_148.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_15.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_157.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_161.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_179.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_18.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_184.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_197.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_200.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_214.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_230.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_239.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_242.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_296.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_318.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_324.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_328.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_337.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_342.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_346.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_350.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_351.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_354 (2).jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_354.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_46.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_467.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_471.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_489.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_505.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_506.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_508.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_511.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_513.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_520.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_523.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_60.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_61.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_76.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_77.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_82.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_85.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Neha/face_93.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Nilesh/\n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_122.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_125.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_133.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_199.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_203.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_241.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_327.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_478.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_515.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_517.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_518.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nilesh/face_527.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Nitin/\n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_240.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_289.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_303.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_319.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_323.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_463.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Nitin/face_487.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Priyanka/\n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_13.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_16.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_20.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_3.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_551.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_71.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Priyanka/face_9.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Promila/\n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_112.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_121.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_126.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_195.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_312.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_341.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_450.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_452.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_456.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_476.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Promila/face_491.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Punit/\n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_102.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_103.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_104.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_105.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_116.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_117.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_118.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_119.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_151.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_155.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_156.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_163.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_168.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_172.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_196.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_238.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_295.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_302.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_305.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_309.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_316.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_322.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_332.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_333.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_343.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_348.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_466.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_83.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_87.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_90.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_96.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Punit/face_99.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Savita/\n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_10.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_14.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_450.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_453.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_466.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_468.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_471.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_473.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_481.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_485.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_560.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_567.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_568.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_569.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_580.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_585.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Savita/face_7.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Simran/\n",
            "  inflating: /content/faceImages/faceImages3/data/Simran/face_460.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Simran/face_468.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Simran/face_472.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Simran/face_484.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Simran/face_531.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Simran/face_95.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Som/\n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_118.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_198.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_237.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_311.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_338.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_344.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Som/face_490.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Veenu/\n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_101.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_115.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_130.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_135.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_297.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_78.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_85.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_89.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Veenu/face_91.jpg  \n",
            "   creating: /content/faceImages/faceImages3/data/Vinita/\n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_21.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_457.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_474.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_551.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_555.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_558.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_562.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/data/Vinita/face_578.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/\n",
            "   creating: /content/faceImages/faceImages3/test/Chirag/\n",
            "  inflating: /content/faceImages/faceImages3/test/Chirag/face_293.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Chirag/face_296.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Chirag/face_297.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Gunjan/\n",
            "  inflating: /content/faceImages/faceImages3/test/Gunjan/face_160.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Gunjan/face_164.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Gunjan/face_169.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Gunjan/face_170.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Jyoti/\n",
            "  inflating: /content/faceImages/faceImages3/test/Jyoti/face_352.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Jyoti/face_451.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Kiran/\n",
            "  inflating: /content/faceImages/faceImages3/test/Kiran/face_355.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Kiran/face_369.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Kiran/face_382.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Kiran/face_387.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Kiran/face_415.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Meghna/\n",
            "  inflating: /content/faceImages/faceImages3/test/Meghna/face_455.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Meghna/face_505.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Meghna/face_552.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Meghna/face_565.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Mohit/\n",
            "  inflating: /content/faceImages/faceImages3/test/Mohit/face_353.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Mohit/face_371.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Mohit/face_381.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Neha/\n",
            "  inflating: /content/faceImages/faceImages3/test/Neha/face_123.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Neha/face_145.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Neha/face_165.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Neha/face_331.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Neha/face_88.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Nilesh/\n",
            "  inflating: /content/faceImages/faceImages3/test/Nilesh/face_139.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Nilesh/face_166.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Nilesh/face_514.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Nilesh/face_528.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Nilesh/face_81.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Nitin/\n",
            "  inflating: /content/faceImages/faceImages3/test/Nitin/face_106.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Nitin/face_301.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Nitin/face_77.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Priyanka/\n",
            "  inflating: /content/faceImages/faceImages3/test/Priyanka/face_454.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Priyanka/face_465.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Promila/\n",
            "  inflating: /content/faceImages/faceImages3/test/Promila/face_213.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Promila/face_488.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Promila/face_95.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Punit/\n",
            "  inflating: /content/faceImages/faceImages3/test/Punit/face_115.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Punit/face_123.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Punit/face_131.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Punit/face_29.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Punit/face_340.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Savita/\n",
            "  inflating: /content/faceImages/faceImages3/test/Savita/face_555.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Savita/face_566.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Savita/face_582.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Savita/face_64.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Simran/\n",
            "  inflating: /content/faceImages/faceImages3/test/Simran/face_153.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Simran/face_162.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Som/\n",
            "  inflating: /content/faceImages/faceImages3/test/Som/face_100.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Som/face_127.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Veenu/\n",
            "  inflating: /content/faceImages/faceImages3/test/Veenu/face_17.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Veenu/face_313.jpg  \n",
            "   creating: /content/faceImages/faceImages3/test/Vinita/\n",
            "  inflating: /content/faceImages/faceImages3/test/Vinita/face_49.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Vinita/face_556.jpg  \n",
            "  inflating: /content/faceImages/faceImages3/test/Vinita/face_573.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CARvV3AAzzp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24a1953d-f794-48da-aeec-186932d7c386"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "faceImages  faceImages3.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBgT9afw4eUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://medium.com/@ODSC/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n",
        "seed_value = 13438"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC65E2Y_BUUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.keras.layers import Embedding\n",
        "#from keras import backend as K\n",
        "#session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "#sess = tf.compat.v1.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "#K.set_session(sess)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b8_bEbb0b0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b21c5b6-0614-4c14-8135-c975cf7031d7"
      },
      "source": [
        "import numpy as np  \n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img  \n",
        "from keras.models import Sequential  \n",
        "from keras.layers import Dropout, Flatten, Dense  \n",
        "from keras import applications  \n",
        "from keras.utils.np_utils import to_categorical  \n",
        "import matplotlib.pyplot as plt  \n",
        "import math  \n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers # https://keras.io/api/layers/initializers/\n",
        "layer = layers.Dense(\n",
        "    units=64,\n",
        "    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
        "    bias_initializer=initializers.Zeros()\n",
        ")\n",
        "\n",
        "import random\n",
        "random.seed(seed_value) \n",
        "\n",
        "from numpy.random import seed\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)  # https://github.com/pytorch/pytorch/issues/11278"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lxfUQ8LaQlA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a1002d15-d78c-43fb-f4c8-f6bb63e29c87"
      },
      "source": [
        "print('Without seed')\n",
        "print(np.random.rand(4))\n",
        "print(np.random.rand(4))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without seed\n",
            "[0.23485932 0.29080151 0.40393218 0.5212769 ]\n",
            "[0.23079737 0.93661399 0.52566437 0.71422455]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muBSJaix85fi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ecd3ed97-dc8b-4ff3-9f7a-2a5e88345967"
      },
      "source": [
        "print('With seed')\n",
        "np.random.seed(seed_value)\n",
        "print(np.random.rand(4))\n",
        "np.random.seed(seed_value)\n",
        "print(np.random.rand(4))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With seed\n",
            "[0.23485932 0.29080151 0.40393218 0.5212769 ]\n",
            "[0.23485932 0.29080151 0.40393218 0.5212769 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Nd5Z2R0gZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We then define couple of parameters,\n",
        "# dimensions of our images.  \n",
        "img_width, img_height = 224, 224  \n",
        "  \n",
        "top_model_weights_path = 'bn_fc_faces_VGG16.hdf5' \n",
        "data_dir = 'faceImages/faceImages3/data'  \n",
        "#validation_data_dir = 'faceImages/faceImages/valid'\n",
        "test_data_dir = 'faceImages/faceImages3/test'\n",
        "  \n",
        "# number of epochs to train top model  \n",
        "epochs = 300  \n",
        "# batch size used by flow_from_directory and predict_generator  \n",
        "batch_size = 64 "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKwj009t0qYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f9a5f292-9d1c-4bc7-ef7c-dff0116db056"
      },
      "source": [
        "## We create the VGG16 model - without the final fully-connected layers (by specifying include_top=False) - and load the ImageNet weights,\n",
        "model = applications.VGG16(include_top=False, weights='imagenet') "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr1O6tfA1Il_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/api/preprocessing/image/\n",
        "# https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92\n",
        "\n",
        "# ImageDataGenerator: Generate batches of tensor image data with real-time data augmentation.\n",
        "# rescale: rescaling factor. \n",
        "#        : multiply the data by the value provided (after applying all other transformations).\n",
        "#flow_from_directory Method : This method will identify classes automatically from the folder name."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvfTR40D05nV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c6636cf-557e-4ae0-b13d-2f36facd3e78"
      },
      "source": [
        "# Save bottlenack features of training data\n",
        "## We then create the data generator for training images, and run them on the VGG16 model to save the bottleneck features for training.\n",
        "## generator.filenames contains all the filenames of the training set. By getting its length, we can get the size of the training set.\n",
        "## generator.class_indices is the map/dictionary for the class-names and their indexes. Getting its length gives us the number of classes.\n",
        "datagen = ImageDataGenerator(validation_split=0.3, rescale=1. / 255)  \n",
        "   \n",
        "generator = datagen.flow_from_directory(\n",
        "    data_dir,  \n",
        "    subset='training',\n",
        "    target_size=(img_width, img_height),  \n",
        "    batch_size=batch_size,  \n",
        "    class_mode=None,  \n",
        "    shuffle=False,\n",
        "    seed = seed_value)  \n",
        "   \n",
        "nb_train_samples = len(generator.filenames)  \n",
        "num_classes = len(generator.class_indices)  \n",
        "   \n",
        "predict_size_train = int(math.ceil(nb_train_samples / batch_size))  \n",
        "   \n",
        "bottleneck_features_train = model.predict_generator(  \n",
        "     generator, predict_size_train)  \n",
        "   \n",
        "np.savez('bn_train_faces_VGG16.npz', bottleneck_features_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 198 images belonging to 17 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oflaA63o1f6t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a573f1f-e377-447f-9023-6686703b707a"
      },
      "source": [
        "# Save bottlenack features of validation data\n",
        "generator = datagen.flow_from_directory(  \n",
        "     data_dir,  \n",
        "     subset='validation',\n",
        "     target_size=(img_width, img_height),  \n",
        "     batch_size=batch_size,  \n",
        "     class_mode=None,  \n",
        "     shuffle=False,\n",
        "     seed = seed_value)  \n",
        "   \n",
        "nb_validation_samples = len(generator.filenames)  \n",
        "   \n",
        "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))  \n",
        "   \n",
        "bottleneck_features_validation = model.predict_generator(  \n",
        "     generator, predict_size_validation)  \n",
        "   \n",
        "np.savez('bn_validation_faces_VGG16.npz', bottleneck_features_validation) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 75 images belonging to 17 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohab3Vg31q-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe4e2a42-dc9d-4772-9f50-c19fb083f521"
      },
      "source": [
        "# Save bottlenack features of test data\n",
        "generator = datagen.flow_from_directory(  \n",
        "     test_data_dir,  \n",
        "     target_size=(img_width, img_height),  \n",
        "     batch_size=batch_size,  \n",
        "     class_mode=None,  \n",
        "     shuffle=False,\n",
        "     seed = seed_value)  \n",
        "   \n",
        "nb_test_samples = len(generator.filenames)  \n",
        "   \n",
        "predict_size_test = int(math.ceil(nb_test_samples / batch_size))  \n",
        "   \n",
        "bottleneck_features_test = model.predict_generator(  \n",
        "     generator, predict_size_test)  \n",
        "   \n",
        "np.savez('bn_test_faces_VGG16.npz', bottleneck_features_test) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 57 images belonging to 17 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNuMsedT1wFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "7f46c17e-61cd-462d-bca4-b725e8fb2527"
      },
      "source": [
        "# convert the training labels to categorical vectors\n",
        "#datagen_top = ImageDataGenerator(validation_split=0.3, rescale=1./255)  \n",
        "generator_top = datagen.flow_from_directory(  \n",
        "         data_dir,\n",
        "         subset='training',  \n",
        "         target_size=(img_width, img_height),  \n",
        "         batch_size=batch_size,  \n",
        "         class_mode='categorical',  \n",
        "         shuffle=False, \n",
        "         seed =seed_value)  \n",
        "   \n",
        "nb_train_samples = len(generator_top.filenames)  \n",
        "num_classes = len(generator_top.class_indices)  \n",
        "print(nb_train_samples)\n",
        "print(num_classes)\n",
        "print(generator_top.class_indices)\n",
        "print(generator_top.filenames)   \n",
        "\n",
        "# load the bottleneck features saved earlier  \n",
        "train_data = np.load('bn_train_faces_VGG16.npz')  \n",
        "   \n",
        "# get the class lebels for the training data, in the original order  \n",
        "train_labels = generator_top.classes \n",
        "print(train_labels)\n",
        "   \n",
        "# convert the training labels to categorical vectors  \n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
        "#print(train_labels[20])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 198 images belonging to 17 classes.\n",
            "198\n",
            "17\n",
            "{'Chirag': 0, 'Gunjan': 1, 'Jyoti': 2, 'Kiran': 3, 'Meghna': 4, 'Mohit': 5, 'Neha': 6, 'Nilesh': 7, 'Nitin': 8, 'Priyanka': 9, 'Promila': 10, 'Punit': 11, 'Savita': 12, 'Simran': 13, 'Som': 14, 'Veenu': 15, 'Vinita': 16}\n",
            "['Chirag/face_300.jpg', 'Chirag/face_301.jpg', 'Chirag/face_305.jpg', 'Chirag/face_306.jpg', 'Chirag/face_317.jpg', 'Chirag/face_462.jpg', 'Gunjan/face_465.jpg', 'Gunjan/face_469.jpg', 'Gunjan/face_473.jpg', 'Gunjan/face_480.jpg', 'Gunjan/face_486.jpg', 'Gunjan/face_504.jpg', 'Gunjan/face_507.jpg', 'Gunjan/face_509.jpg', 'Gunjan/face_510.jpg', 'Gunjan/face_512.jpg', 'Gunjan/face_519.jpg', 'Gunjan/face_76.jpg', 'Jyoti/face_474.jpg', 'Jyoti/face_475.jpg', 'Jyoti/face_477.jpg', 'Jyoti/face_479.jpg', 'Jyoti/face_485.jpg', 'Jyoti/face_487.jpg', 'Jyoti/face_84.jpg', 'Kiran/face_373.jpg', 'Kiran/face_375.jpg', 'Kiran/face_379.jpg', 'Kiran/face_380.jpg', 'Kiran/face_384.jpg', 'Kiran/face_385.jpg', 'Kiran/face_400.jpg', 'Kiran/face_404.jpg', 'Kiran/face_405.jpg', 'Kiran/face_407.jpg', 'Kiran/face_409.jpg', 'Kiran/face_411.jpg', 'Kiran/face_413.jpg', 'Kiran/face_419.jpg', 'Kiran/face_423.jpg', 'Kiran/face_428.jpg', 'Kiran/face_434.jpg', 'Kiran/face_478.jpg', 'Kiran/face_86.jpg', 'Meghna/face_503.jpg', 'Meghna/face_504.jpg', 'Meghna/face_550.jpg', 'Meghna/face_553.jpg', 'Meghna/face_554.jpg', 'Meghna/face_557.jpg', 'Meghna/face_559.jpg', 'Meghna/face_561.jpg', 'Meghna/face_563.jpg', 'Meghna/face_564.jpg', 'Meghna/face_567.jpg', 'Meghna/face_571.jpg', 'Meghna/face_572.jpg', 'Meghna/face_579.jpg', 'Meghna/face_580.jpg', 'Meghna/face_581.jpg', 'Meghna/face_583.jpg', 'Meghna/face_587.jpg', 'Meghna/face_591.jpg', 'Mohit/face_362.jpg', 'Mohit/face_367.jpg', 'Mohit/face_372.jpg', 'Mohit/face_378.jpg', 'Mohit/face_403.jpg', 'Mohit/face_406.jpg', 'Mohit/face_425.jpg', 'Neha/face_141.jpg', 'Neha/face_148.jpg', 'Neha/face_15.jpg', 'Neha/face_157.jpg', 'Neha/face_161.jpg', 'Neha/face_179.jpg', 'Neha/face_18.jpg', 'Neha/face_184.jpg', 'Neha/face_197.jpg', 'Neha/face_200.jpg', 'Neha/face_214.jpg', 'Neha/face_230.jpg', 'Neha/face_239.jpg', 'Neha/face_242.jpg', 'Neha/face_296.jpg', 'Neha/face_318.jpg', 'Neha/face_324.jpg', 'Neha/face_328.jpg', 'Neha/face_337.jpg', 'Neha/face_342.jpg', 'Neha/face_346.jpg', 'Neha/face_350.jpg', 'Neha/face_351.jpg', 'Neha/face_354 (2).jpg', 'Neha/face_354.jpg', 'Neha/face_46.jpg', 'Neha/face_467.jpg', 'Neha/face_471.jpg', 'Neha/face_489.jpg', 'Neha/face_505.jpg', 'Neha/face_506.jpg', 'Neha/face_508.jpg', 'Neha/face_511.jpg', 'Neha/face_513.jpg', 'Neha/face_520.jpg', 'Neha/face_523.jpg', 'Neha/face_60.jpg', 'Neha/face_61.jpg', 'Neha/face_76.jpg', 'Neha/face_77.jpg', 'Neha/face_82.jpg', 'Neha/face_85.jpg', 'Neha/face_93.jpg', 'Nilesh/face_199.jpg', 'Nilesh/face_203.jpg', 'Nilesh/face_241.jpg', 'Nilesh/face_327.jpg', 'Nilesh/face_478.jpg', 'Nilesh/face_515.jpg', 'Nilesh/face_517.jpg', 'Nilesh/face_518.jpg', 'Nilesh/face_527.jpg', 'Nitin/face_303.jpg', 'Nitin/face_319.jpg', 'Nitin/face_323.jpg', 'Nitin/face_463.jpg', 'Nitin/face_487.jpg', 'Priyanka/face_20.jpg', 'Priyanka/face_3.jpg', 'Priyanka/face_551.jpg', 'Priyanka/face_71.jpg', 'Priyanka/face_9.jpg', 'Promila/face_195.jpg', 'Promila/face_312.jpg', 'Promila/face_341.jpg', 'Promila/face_450.jpg', 'Promila/face_452.jpg', 'Promila/face_456.jpg', 'Promila/face_476.jpg', 'Promila/face_491.jpg', 'Punit/face_155.jpg', 'Punit/face_156.jpg', 'Punit/face_163.jpg', 'Punit/face_168.jpg', 'Punit/face_172.jpg', 'Punit/face_196.jpg', 'Punit/face_238.jpg', 'Punit/face_295.jpg', 'Punit/face_302.jpg', 'Punit/face_305.jpg', 'Punit/face_309.jpg', 'Punit/face_316.jpg', 'Punit/face_322.jpg', 'Punit/face_332.jpg', 'Punit/face_333.jpg', 'Punit/face_343.jpg', 'Punit/face_348.jpg', 'Punit/face_466.jpg', 'Punit/face_83.jpg', 'Punit/face_87.jpg', 'Punit/face_90.jpg', 'Punit/face_96.jpg', 'Punit/face_99.jpg', 'Savita/face_468.jpg', 'Savita/face_471.jpg', 'Savita/face_473.jpg', 'Savita/face_481.jpg', 'Savita/face_485.jpg', 'Savita/face_560.jpg', 'Savita/face_567.jpg', 'Savita/face_568.jpg', 'Savita/face_569.jpg', 'Savita/face_580.jpg', 'Savita/face_585.jpg', 'Savita/face_7.jpg', 'Simran/face_468.jpg', 'Simran/face_472.jpg', 'Simran/face_484.jpg', 'Simran/face_531.jpg', 'Simran/face_95.jpg', 'Som/face_237.jpg', 'Som/face_311.jpg', 'Som/face_338.jpg', 'Som/face_344.jpg', 'Som/face_490.jpg', 'Veenu/face_130.jpg', 'Veenu/face_135.jpg', 'Veenu/face_297.jpg', 'Veenu/face_78.jpg', 'Veenu/face_85.jpg', 'Veenu/face_89.jpg', 'Veenu/face_91.jpg', 'Vinita/face_474.jpg', 'Vinita/face_551.jpg', 'Vinita/face_555.jpg', 'Vinita/face_558.jpg', 'Vinita/face_562.jpg', 'Vinita/face_578.jpg']\n",
            "[ 0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2\n",
            "  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4\n",
            "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  6  6\n",
            "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
            "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7\n",
            "  7  7  8  8  8  8  8  9  9  9  9  9 10 10 10 10 10 10 10 10 11 11 11 11\n",
            " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12\n",
            " 12 12 12 12 12 12 12 13 13 13 13 13 14 14 14 14 14 15 15 15 15 15 15 15\n",
            " 16 16 16 16 16 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ofORAU19Sc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9d1a2536-8ff6-4f5a-d00b-0c5316d98898"
      },
      "source": [
        "#convert the validation labels to categorical vectors\n",
        "generator_top = datagen.flow_from_directory(  \n",
        "        data_dir, \n",
        "        subset='validation', \n",
        "         target_size=(img_width, img_height),  \n",
        "         batch_size=batch_size,  \n",
        "         class_mode=None,  \n",
        "         shuffle=False, \n",
        "         seed = seed_value)  \n",
        "   \n",
        "nb_validation_samples = len(generator_top.filenames)  \n",
        "  \n",
        "validation_data = np.load('bn_validation_faces_VGG16.npz')  \n",
        "   \n",
        "validation_labels = generator_top.classes  \n",
        "print(validation_labels) \n",
        "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
        "#print(validation_labels) \n",
        "print(nb_validation_samples)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 75 images belonging to 17 classes.\n",
            "[ 0  0  1  1  1  1  1  2  2  2  3  3  3  3  3  3  3  4  4  4  4  4  4  4\n",
            "  5  5  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  8\n",
            "  8  9  9 10 10 10 11 11 11 11 11 11 11 11 11 12 12 12 12 12 13 14 14 15\n",
            " 15 16 16]\n",
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vffgx2nL2AJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5970b060-eab4-4b5a-cb71-e256f4053cf6"
      },
      "source": [
        "#convert the test labels to categorical vectors\n",
        "generator_top = datagen.flow_from_directory(  \n",
        "        test_data_dir,  \n",
        "         target_size=(img_width, img_height),  \n",
        "         batch_size=batch_size,  \n",
        "         class_mode=None,  \n",
        "         shuffle=False, \n",
        "         seed = seed_value)  \n",
        "   \n",
        "nb_test_samples = len(generator_top.filenames)  \n",
        "  \n",
        "test_data = np.load('bn_test_faces_VGG16.npz')  \n",
        "   \n",
        "test_labels = generator_top.classes\n",
        "print(test_labels)   \n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 57 images belonging to 17 classes.\n",
            "[ 0  0  0  1  1  1  1  2  2  3  3  3  3  3  4  4  4  4  5  5  5  6  6  6\n",
            "  6  6  7  7  7  7  7  8  8  8  9  9 10 10 10 11 11 11 11 11 12 12 12 12\n",
            " 13 13 14 14 15 15 16 16 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaVrpMQy2DCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "debd3f5a-9bf1-4c99-8e31-60500567504e"
      },
      "source": [
        "# Description of npz files\n",
        "train_data.files, validation_data.files, test_data.files, train_data['arr_0'].shape, validation_data['arr_0'].shape, test_data['arr_0'].shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['arr_0'],\n",
              " ['arr_0'],\n",
              " ['arr_0'],\n",
              " (198, 7, 7, 512),\n",
              " (75, 7, 7, 512),\n",
              " (57, 7, 7, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7xUD8stA1jd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da7e67f2-bba8-40d6-9809-31acc896be02"
      },
      "source": [
        "# Description of labels of dataset\n",
        "train_labels.shape, validation_labels.shape, test_labels.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((198, 17), (75, 17), (57, 17))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lssPfiBi2HBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "8fadbc08-8757-43ac-b602-3553a7b0f602"
      },
      "source": [
        "### Define your architecture.\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(GlobalAveragePooling2D(input_shape=train_data['arr_0'].shape[1:]))\n",
        "#model.add(Dense(400, activation='relu')) \n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5, seed=seed_value))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 17)                4369      \n",
            "=================================================================\n",
            "Total params: 136,721\n",
            "Trainable params: 136,209\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNpg2dIQ2MtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b27d8042-5186-4e1e-f058-44c70987562b"
      },
      "source": [
        "### Compile the model.\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "### Train the model.\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "checkpointer = ModelCheckpoint(filepath=top_model_weights_path, \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(train_data['arr_0'], train_labels,  \n",
        "          epochs=epochs,  \n",
        "          batch_size=batch_size,  \n",
        "          validation_data=(validation_data['arr_0'], validation_labels),callbacks=[checkpointer], verbose=1, shuffle=True)  \n",
        "   \n",
        "#model.save_weights(top_model_weights_path)  \n",
        "   \n",
        "(eval_loss, eval_accuracy) = model.evaluate(  \n",
        "     validation_data['arr_0'], validation_labels, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
        "print(\"[INFO] Loss: {}\".format(eval_loss))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 198 samples, validate on 75 samples\n",
            "Epoch 1/300\n",
            "198/198 [==============================] - 0s 2ms/step - loss: 3.3641 - accuracy: 0.0808 - val_loss: 3.1072 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.10722, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 2/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 2.7412 - accuracy: 0.1364 - val_loss: 3.0231 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.10722 to 3.02312, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 3/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 2.3401 - accuracy: 0.2879 - val_loss: 2.9442 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.02312 to 2.94418, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 4/300\n",
            "198/198 [==============================] - 0s 193us/step - loss: 2.1141 - accuracy: 0.3434 - val_loss: 2.8821 - val_accuracy: 0.0533\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.94418 to 2.88213, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 5/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 1.9277 - accuracy: 0.4293 - val_loss: 2.8309 - val_accuracy: 0.0533\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.88213 to 2.83087, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 6/300\n",
            "198/198 [==============================] - 0s 363us/step - loss: 1.8859 - accuracy: 0.4141 - val_loss: 2.7954 - val_accuracy: 0.0400\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.83087 to 2.79542, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 7/300\n",
            "198/198 [==============================] - 0s 189us/step - loss: 1.7068 - accuracy: 0.4949 - val_loss: 2.7764 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.79542 to 2.77643, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 8/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 1.6307 - accuracy: 0.5101 - val_loss: 2.7488 - val_accuracy: 0.0400\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.77643 to 2.74884, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 9/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 1.4387 - accuracy: 0.5556 - val_loss: 2.7163 - val_accuracy: 0.0400\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.74884 to 2.71627, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 10/300\n",
            "198/198 [==============================] - 0s 194us/step - loss: 1.3397 - accuracy: 0.6061 - val_loss: 2.6966 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.71627 to 2.69657, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 11/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 1.2752 - accuracy: 0.5859 - val_loss: 2.6901 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.69657 to 2.69009, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 12/300\n",
            "198/198 [==============================] - 0s 262us/step - loss: 1.2086 - accuracy: 0.6566 - val_loss: 2.6855 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.69009 to 2.68546, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 13/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 1.1521 - accuracy: 0.7121 - val_loss: 2.6783 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.68546 to 2.67829, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 14/300\n",
            "198/198 [==============================] - 0s 211us/step - loss: 1.0114 - accuracy: 0.7172 - val_loss: 2.6614 - val_accuracy: 0.0267\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.67829 to 2.66143, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 15/300\n",
            "198/198 [==============================] - 0s 198us/step - loss: 1.0072 - accuracy: 0.7172 - val_loss: 2.6172 - val_accuracy: 0.1067\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.66143 to 2.61725, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 16/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.9160 - accuracy: 0.7374 - val_loss: 2.5474 - val_accuracy: 0.2133\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.61725 to 2.54743, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 17/300\n",
            "198/198 [==============================] - 0s 187us/step - loss: 0.9201 - accuracy: 0.7020 - val_loss: 2.4938 - val_accuracy: 0.2800\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.54743 to 2.49380, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 18/300\n",
            "198/198 [==============================] - 0s 193us/step - loss: 0.9046 - accuracy: 0.7222 - val_loss: 2.4637 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.49380 to 2.46375, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 19/300\n",
            "198/198 [==============================] - 0s 203us/step - loss: 0.8204 - accuracy: 0.7677 - val_loss: 2.4430 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.46375 to 2.44304, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 20/300\n",
            "198/198 [==============================] - 0s 217us/step - loss: 0.8119 - accuracy: 0.7677 - val_loss: 2.4303 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.44304 to 2.43030, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 21/300\n",
            "198/198 [==============================] - 0s 200us/step - loss: 0.7596 - accuracy: 0.7929 - val_loss: 2.4242 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.43030 to 2.42424, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 22/300\n",
            "198/198 [==============================] - 0s 190us/step - loss: 0.8099 - accuracy: 0.8030 - val_loss: 2.4154 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.42424 to 2.41541, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 23/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.6927 - accuracy: 0.8434 - val_loss: 2.4037 - val_accuracy: 0.3200\n",
            "\n",
            "Epoch 00023: val_loss improved from 2.41541 to 2.40373, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 24/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.6425 - accuracy: 0.8434 - val_loss: 2.3862 - val_accuracy: 0.2800\n",
            "\n",
            "Epoch 00024: val_loss improved from 2.40373 to 2.38615, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 25/300\n",
            "198/198 [==============================] - 0s 200us/step - loss: 0.6562 - accuracy: 0.8687 - val_loss: 2.3778 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.38615 to 2.37776, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 26/300\n",
            "198/198 [==============================] - 0s 250us/step - loss: 0.6068 - accuracy: 0.8990 - val_loss: 2.3620 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.37776 to 2.36202, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 27/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.6214 - accuracy: 0.8636 - val_loss: 2.3382 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.36202 to 2.33824, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 28/300\n",
            "198/198 [==============================] - 0s 187us/step - loss: 0.5533 - accuracy: 0.8788 - val_loss: 2.3160 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.33824 to 2.31596, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 29/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.4901 - accuracy: 0.8889 - val_loss: 2.2998 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.31596 to 2.29977, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 30/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.4971 - accuracy: 0.8990 - val_loss: 2.2823 - val_accuracy: 0.3067\n",
            "\n",
            "Epoch 00030: val_loss improved from 2.29977 to 2.28230, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 31/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.5172 - accuracy: 0.8788 - val_loss: 2.2717 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00031: val_loss improved from 2.28230 to 2.27175, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 32/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.5096 - accuracy: 0.8586 - val_loss: 2.2572 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00032: val_loss improved from 2.27175 to 2.25722, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 33/300\n",
            "198/198 [==============================] - 0s 193us/step - loss: 0.4606 - accuracy: 0.8889 - val_loss: 2.2425 - val_accuracy: 0.3200\n",
            "\n",
            "Epoch 00033: val_loss improved from 2.25722 to 2.24251, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 34/300\n",
            "198/198 [==============================] - 0s 191us/step - loss: 0.4043 - accuracy: 0.9343 - val_loss: 2.2329 - val_accuracy: 0.3200\n",
            "\n",
            "Epoch 00034: val_loss improved from 2.24251 to 2.23291, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 35/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.3875 - accuracy: 0.9242 - val_loss: 2.2298 - val_accuracy: 0.3200\n",
            "\n",
            "Epoch 00035: val_loss improved from 2.23291 to 2.22977, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 36/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.4070 - accuracy: 0.9293 - val_loss: 2.2316 - val_accuracy: 0.3200\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 2.22977\n",
            "Epoch 37/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.4003 - accuracy: 0.9394 - val_loss: 2.2289 - val_accuracy: 0.2800\n",
            "\n",
            "Epoch 00037: val_loss improved from 2.22977 to 2.22892, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 38/300\n",
            "198/198 [==============================] - 0s 185us/step - loss: 0.3705 - accuracy: 0.9343 - val_loss: 2.2286 - val_accuracy: 0.2533\n",
            "\n",
            "Epoch 00038: val_loss improved from 2.22892 to 2.22862, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 39/300\n",
            "198/198 [==============================] - 0s 203us/step - loss: 0.3789 - accuracy: 0.9192 - val_loss: 2.2541 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 2.22862\n",
            "Epoch 40/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.3850 - accuracy: 0.9444 - val_loss: 2.2834 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 2.22862\n",
            "Epoch 41/300\n",
            "198/198 [==============================] - 0s 166us/step - loss: 0.3080 - accuracy: 0.9646 - val_loss: 2.3040 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 2.22862\n",
            "Epoch 42/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.3115 - accuracy: 0.9646 - val_loss: 2.2933 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 2.22862\n",
            "Epoch 43/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.3421 - accuracy: 0.9394 - val_loss: 2.2470 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 2.22862\n",
            "Epoch 44/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.2841 - accuracy: 0.9697 - val_loss: 2.1880 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00044: val_loss improved from 2.22862 to 2.18805, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 45/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.2702 - accuracy: 0.9697 - val_loss: 2.1603 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00045: val_loss improved from 2.18805 to 2.16027, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 46/300\n",
            "198/198 [==============================] - 0s 189us/step - loss: 0.2985 - accuracy: 0.9293 - val_loss: 2.1599 - val_accuracy: 0.2800\n",
            "\n",
            "Epoch 00046: val_loss improved from 2.16027 to 2.15987, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 47/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.3315 - accuracy: 0.9293 - val_loss: 2.1667 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 2.15987\n",
            "Epoch 48/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.2572 - accuracy: 0.9646 - val_loss: 2.1909 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 2.15987\n",
            "Epoch 49/300\n",
            "198/198 [==============================] - 0s 165us/step - loss: 0.2434 - accuracy: 0.9646 - val_loss: 2.2133 - val_accuracy: 0.2400\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 2.15987\n",
            "Epoch 50/300\n",
            "198/198 [==============================] - 0s 167us/step - loss: 0.2804 - accuracy: 0.9444 - val_loss: 2.1785 - val_accuracy: 0.2533\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 2.15987\n",
            "Epoch 51/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.3156 - accuracy: 0.9394 - val_loss: 2.1166 - val_accuracy: 0.2533\n",
            "\n",
            "Epoch 00051: val_loss improved from 2.15987 to 2.11657, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 52/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.2859 - accuracy: 0.9394 - val_loss: 2.0465 - val_accuracy: 0.2933\n",
            "\n",
            "Epoch 00052: val_loss improved from 2.11657 to 2.04650, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 53/300\n",
            "198/198 [==============================] - 0s 209us/step - loss: 0.2255 - accuracy: 0.9697 - val_loss: 2.0067 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00053: val_loss improved from 2.04650 to 2.00667, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 54/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.2321 - accuracy: 0.9697 - val_loss: 1.9937 - val_accuracy: 0.3867\n",
            "\n",
            "Epoch 00054: val_loss improved from 2.00667 to 1.99373, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 55/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.2250 - accuracy: 0.9646 - val_loss: 1.9777 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00055: val_loss improved from 1.99373 to 1.97770, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 56/300\n",
            "198/198 [==============================] - 0s 183us/step - loss: 0.1735 - accuracy: 0.9747 - val_loss: 1.9660 - val_accuracy: 0.3733\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.97770 to 1.96598, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 57/300\n",
            "198/198 [==============================] - 0s 225us/step - loss: 0.1710 - accuracy: 0.9848 - val_loss: 1.9673 - val_accuracy: 0.4133\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.96598\n",
            "Epoch 58/300\n",
            "198/198 [==============================] - 0s 267us/step - loss: 0.2724 - accuracy: 0.9444 - val_loss: 1.9679 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.96598\n",
            "Epoch 59/300\n",
            "198/198 [==============================] - 0s 190us/step - loss: 0.2227 - accuracy: 0.9646 - val_loss: 1.9670 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.96598\n",
            "Epoch 60/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.1830 - accuracy: 0.9747 - val_loss: 1.9546 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00060: val_loss improved from 1.96598 to 1.95463, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 61/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.1753 - accuracy: 0.9949 - val_loss: 1.9319 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00061: val_loss improved from 1.95463 to 1.93190, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 62/300\n",
            "198/198 [==============================] - 0s 206us/step - loss: 0.1526 - accuracy: 0.9848 - val_loss: 1.9159 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00062: val_loss improved from 1.93190 to 1.91589, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 63/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.1683 - accuracy: 0.9798 - val_loss: 1.8989 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00063: val_loss improved from 1.91589 to 1.89886, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 64/300\n",
            "198/198 [==============================] - 0s 192us/step - loss: 0.1997 - accuracy: 0.9848 - val_loss: 1.8764 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00064: val_loss improved from 1.89886 to 1.87639, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 65/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.2034 - accuracy: 0.9747 - val_loss: 1.8873 - val_accuracy: 0.3733\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.87639\n",
            "Epoch 66/300\n",
            "198/198 [==============================] - 0s 185us/step - loss: 0.1763 - accuracy: 0.9848 - val_loss: 1.8970 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.87639\n",
            "Epoch 67/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.1566 - accuracy: 0.9798 - val_loss: 1.8916 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.87639\n",
            "Epoch 68/300\n",
            "198/198 [==============================] - 0s 193us/step - loss: 0.1551 - accuracy: 0.9697 - val_loss: 1.8686 - val_accuracy: 0.3867\n",
            "\n",
            "Epoch 00068: val_loss improved from 1.87639 to 1.86859, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 69/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.1634 - accuracy: 0.9798 - val_loss: 1.8586 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00069: val_loss improved from 1.86859 to 1.85859, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 70/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.1654 - accuracy: 0.9798 - val_loss: 1.8564 - val_accuracy: 0.3733\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.85859 to 1.85640, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 71/300\n",
            "198/198 [==============================] - 0s 192us/step - loss: 0.1455 - accuracy: 0.9848 - val_loss: 1.8551 - val_accuracy: 0.3867\n",
            "\n",
            "Epoch 00071: val_loss improved from 1.85640 to 1.85506, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 72/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.1435 - accuracy: 0.9848 - val_loss: 1.8681 - val_accuracy: 0.3867\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.85506\n",
            "Epoch 73/300\n",
            "198/198 [==============================] - 0s 203us/step - loss: 0.1428 - accuracy: 0.9949 - val_loss: 1.8677 - val_accuracy: 0.3867\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.85506\n",
            "Epoch 74/300\n",
            "198/198 [==============================] - 0s 190us/step - loss: 0.1598 - accuracy: 0.9899 - val_loss: 1.8546 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.85506 to 1.85460, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 75/300\n",
            "198/198 [==============================] - 0s 187us/step - loss: 0.1456 - accuracy: 0.9848 - val_loss: 1.8814 - val_accuracy: 0.3867\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.85460\n",
            "Epoch 76/300\n",
            "198/198 [==============================] - 0s 225us/step - loss: 0.1331 - accuracy: 0.9798 - val_loss: 1.9576 - val_accuracy: 0.3467\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.85460\n",
            "Epoch 77/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.1237 - accuracy: 0.9899 - val_loss: 1.9555 - val_accuracy: 0.3467\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.85460\n",
            "Epoch 78/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.1064 - accuracy: 0.9949 - val_loss: 1.9339 - val_accuracy: 0.3467\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.85460\n",
            "Epoch 79/300\n",
            "198/198 [==============================] - 0s 188us/step - loss: 0.1060 - accuracy: 0.9949 - val_loss: 1.8951 - val_accuracy: 0.3733\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.85460\n",
            "Epoch 80/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.1186 - accuracy: 0.9899 - val_loss: 1.8608 - val_accuracy: 0.3733\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.85460\n",
            "Epoch 81/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.1135 - accuracy: 0.9949 - val_loss: 1.8441 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00081: val_loss improved from 1.85460 to 1.84407, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 82/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.1284 - accuracy: 0.9798 - val_loss: 1.8256 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00082: val_loss improved from 1.84407 to 1.82562, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 83/300\n",
            "198/198 [==============================] - 0s 199us/step - loss: 0.0951 - accuracy: 0.9949 - val_loss: 1.8236 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00083: val_loss improved from 1.82562 to 1.82360, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 84/300\n",
            "198/198 [==============================] - 0s 207us/step - loss: 0.1283 - accuracy: 0.9899 - val_loss: 1.8067 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00084: val_loss improved from 1.82360 to 1.80670, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 85/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.1050 - accuracy: 0.9949 - val_loss: 1.7819 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00085: val_loss improved from 1.80670 to 1.78185, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 86/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0979 - accuracy: 0.9949 - val_loss: 1.7288 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00086: val_loss improved from 1.78185 to 1.72884, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 87/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.1020 - accuracy: 0.9949 - val_loss: 1.6786 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00087: val_loss improved from 1.72884 to 1.67855, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 88/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.1091 - accuracy: 0.9899 - val_loss: 1.6592 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00088: val_loss improved from 1.67855 to 1.65919, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 89/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.1105 - accuracy: 0.9848 - val_loss: 1.6508 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00089: val_loss improved from 1.65919 to 1.65084, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 90/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.1080 - accuracy: 0.9848 - val_loss: 1.6530 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.65084\n",
            "Epoch 91/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0923 - accuracy: 0.9899 - val_loss: 1.6636 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.65084\n",
            "Epoch 92/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0939 - accuracy: 0.9848 - val_loss: 1.6776 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.65084\n",
            "Epoch 93/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 1.6837 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.65084\n",
            "Epoch 94/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0738 - accuracy: 1.0000 - val_loss: 1.6967 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.65084\n",
            "Epoch 95/300\n",
            "198/198 [==============================] - 0s 209us/step - loss: 0.0855 - accuracy: 0.9949 - val_loss: 1.6933 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.65084\n",
            "Epoch 96/300\n",
            "198/198 [==============================] - 0s 210us/step - loss: 0.0895 - accuracy: 0.9899 - val_loss: 1.6779 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.65084\n",
            "Epoch 97/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 1.6661 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.65084\n",
            "Epoch 98/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.1055 - accuracy: 0.9798 - val_loss: 1.6611 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.65084\n",
            "Epoch 99/300\n",
            "198/198 [==============================] - 0s 195us/step - loss: 0.0847 - accuracy: 1.0000 - val_loss: 1.6594 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.65084\n",
            "Epoch 100/300\n",
            "198/198 [==============================] - 0s 189us/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 1.6641 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.65084\n",
            "Epoch 101/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0688 - accuracy: 0.9949 - val_loss: 1.6672 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.65084\n",
            "Epoch 102/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0996 - accuracy: 0.9949 - val_loss: 1.6607 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.65084\n",
            "Epoch 103/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0720 - accuracy: 1.0000 - val_loss: 1.6728 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.65084\n",
            "Epoch 104/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0703 - accuracy: 0.9949 - val_loss: 1.6635 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.65084\n",
            "Epoch 105/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.0832 - accuracy: 0.9949 - val_loss: 1.6530 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.65084\n",
            "Epoch 106/300\n",
            "198/198 [==============================] - 0s 197us/step - loss: 0.0889 - accuracy: 0.9848 - val_loss: 1.6395 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00106: val_loss improved from 1.65084 to 1.63953, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 107/300\n",
            "198/198 [==============================] - 0s 189us/step - loss: 0.0845 - accuracy: 1.0000 - val_loss: 1.6115 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00107: val_loss improved from 1.63953 to 1.61153, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 108/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0776 - accuracy: 1.0000 - val_loss: 1.5709 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00108: val_loss improved from 1.61153 to 1.57093, saving model to bn_fc_faces_VGG16.hdf5\n",
            "Epoch 109/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 1.5859 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.57093\n",
            "Epoch 110/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0740 - accuracy: 0.9949 - val_loss: 1.6105 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.57093\n",
            "Epoch 111/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0700 - accuracy: 1.0000 - val_loss: 1.6319 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.57093\n",
            "Epoch 112/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 1.6434 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.57093\n",
            "Epoch 113/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0612 - accuracy: 0.9949 - val_loss: 1.6448 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.57093\n",
            "Epoch 114/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0733 - accuracy: 0.9949 - val_loss: 1.6401 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.57093\n",
            "Epoch 115/300\n",
            "198/198 [==============================] - 0s 215us/step - loss: 0.0576 - accuracy: 1.0000 - val_loss: 1.6638 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.57093\n",
            "Epoch 116/300\n",
            "198/198 [==============================] - 0s 216us/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 1.7005 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.57093\n",
            "Epoch 117/300\n",
            "198/198 [==============================] - 0s 196us/step - loss: 0.0552 - accuracy: 0.9949 - val_loss: 1.6966 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.57093\n",
            "Epoch 118/300\n",
            "198/198 [==============================] - 0s 228us/step - loss: 0.0538 - accuracy: 0.9949 - val_loss: 1.7047 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.57093\n",
            "Epoch 119/300\n",
            "198/198 [==============================] - 0s 216us/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 1.7273 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.57093\n",
            "Epoch 120/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0784 - accuracy: 0.9949 - val_loss: 1.7230 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.57093\n",
            "Epoch 121/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0891 - accuracy: 0.9949 - val_loss: 1.7891 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.57093\n",
            "Epoch 122/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0661 - accuracy: 0.9899 - val_loss: 1.8571 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.57093\n",
            "Epoch 123/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0681 - accuracy: 0.9848 - val_loss: 1.8892 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.57093\n",
            "Epoch 124/300\n",
            "198/198 [==============================] - 0s 168us/step - loss: 0.0672 - accuracy: 0.9899 - val_loss: 1.8725 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.57093\n",
            "Epoch 125/300\n",
            "198/198 [==============================] - 0s 191us/step - loss: 0.0702 - accuracy: 0.9899 - val_loss: 1.8673 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.57093\n",
            "Epoch 126/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 1.8803 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.57093\n",
            "Epoch 127/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0827 - accuracy: 0.9848 - val_loss: 1.8744 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.57093\n",
            "Epoch 128/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0796 - accuracy: 0.9899 - val_loss: 1.8978 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.57093\n",
            "Epoch 129/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0769 - accuracy: 0.9848 - val_loss: 2.0012 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.57093\n",
            "Epoch 130/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 2.0463 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.57093\n",
            "Epoch 131/300\n",
            "198/198 [==============================] - 0s 185us/step - loss: 0.0518 - accuracy: 0.9949 - val_loss: 1.9840 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.57093\n",
            "Epoch 132/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 1.9141 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.57093\n",
            "Epoch 133/300\n",
            "198/198 [==============================] - 0s 192us/step - loss: 0.0534 - accuracy: 0.9949 - val_loss: 1.8512 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.57093\n",
            "Epoch 134/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0560 - accuracy: 0.9949 - val_loss: 1.9342 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.57093\n",
            "Epoch 135/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0598 - accuracy: 1.0000 - val_loss: 2.1373 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.57093\n",
            "Epoch 136/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 2.2349 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.57093\n",
            "Epoch 137/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0650 - accuracy: 0.9949 - val_loss: 2.1895 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.57093\n",
            "Epoch 138/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0703 - accuracy: 0.9798 - val_loss: 2.0515 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.57093\n",
            "Epoch 139/300\n",
            "198/198 [==============================] - 0s 188us/step - loss: 0.0808 - accuracy: 0.9848 - val_loss: 1.9211 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.57093\n",
            "Epoch 140/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.0754 - accuracy: 0.9798 - val_loss: 1.8339 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.57093\n",
            "Epoch 141/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0625 - accuracy: 0.9899 - val_loss: 1.9185 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.57093\n",
            "Epoch 142/300\n",
            "198/198 [==============================] - 0s 200us/step - loss: 0.0733 - accuracy: 0.9848 - val_loss: 1.9966 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.57093\n",
            "Epoch 143/300\n",
            "198/198 [==============================] - 0s 217us/step - loss: 0.0817 - accuracy: 0.9848 - val_loss: 2.0200 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.57093\n",
            "Epoch 144/300\n",
            "198/198 [==============================] - 0s 197us/step - loss: 0.0790 - accuracy: 0.9848 - val_loss: 1.9691 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.57093\n",
            "Epoch 145/300\n",
            "198/198 [==============================] - 0s 197us/step - loss: 0.0812 - accuracy: 0.9949 - val_loss: 1.9303 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.57093\n",
            "Epoch 146/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0601 - accuracy: 0.9899 - val_loss: 2.0614 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.57093\n",
            "Epoch 147/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0520 - accuracy: 1.0000 - val_loss: 2.2324 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.57093\n",
            "Epoch 148/300\n",
            "198/198 [==============================] - 0s 191us/step - loss: 0.0743 - accuracy: 0.9848 - val_loss: 2.2738 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.57093\n",
            "Epoch 149/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 2.1944 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.57093\n",
            "Epoch 150/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0508 - accuracy: 0.9949 - val_loss: 2.0918 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.57093\n",
            "Epoch 151/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0581 - accuracy: 0.9899 - val_loss: 2.0041 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.57093\n",
            "Epoch 152/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0639 - accuracy: 0.9899 - val_loss: 1.9243 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.57093\n",
            "Epoch 153/300\n",
            "198/198 [==============================] - 0s 192us/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 1.8809 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.57093\n",
            "Epoch 154/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 2.0524 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.57093\n",
            "Epoch 155/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.0552 - accuracy: 0.9899 - val_loss: 2.1613 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.57093\n",
            "Epoch 156/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0628 - accuracy: 0.9899 - val_loss: 2.1467 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.57093\n",
            "Epoch 157/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0478 - accuracy: 0.9949 - val_loss: 2.0628 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.57093\n",
            "Epoch 158/300\n",
            "198/198 [==============================] - 0s 187us/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 1.9810 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.57093\n",
            "Epoch 159/300\n",
            "198/198 [==============================] - 0s 193us/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 1.9189 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.57093\n",
            "Epoch 160/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 1.8090 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.57093\n",
            "Epoch 161/300\n",
            "198/198 [==============================] - 0s 166us/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 1.7465 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.57093\n",
            "Epoch 162/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0475 - accuracy: 0.9949 - val_loss: 1.7515 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.57093\n",
            "Epoch 163/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 1.8470 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.57093\n",
            "Epoch 164/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0485 - accuracy: 0.9899 - val_loss: 1.8413 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.57093\n",
            "Epoch 165/300\n",
            "198/198 [==============================] - 0s 193us/step - loss: 0.0423 - accuracy: 1.0000 - val_loss: 1.8107 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.57093\n",
            "Epoch 166/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 1.7735 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.57093\n",
            "Epoch 167/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 1.7594 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.57093\n",
            "Epoch 168/300\n",
            "198/198 [==============================] - 0s 206us/step - loss: 0.0407 - accuracy: 0.9949 - val_loss: 1.7587 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.57093\n",
            "Epoch 169/300\n",
            "198/198 [==============================] - 0s 200us/step - loss: 0.0334 - accuracy: 0.9949 - val_loss: 1.7715 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.57093\n",
            "Epoch 170/300\n",
            "198/198 [==============================] - 0s 225us/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 1.7905 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.57093\n",
            "Epoch 171/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0350 - accuracy: 0.9949 - val_loss: 1.8001 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.57093\n",
            "Epoch 172/300\n",
            "198/198 [==============================] - 0s 165us/step - loss: 0.0430 - accuracy: 0.9949 - val_loss: 1.7864 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.57093\n",
            "Epoch 173/300\n",
            "198/198 [==============================] - 0s 167us/step - loss: 0.0525 - accuracy: 0.9949 - val_loss: 1.7686 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.57093\n",
            "Epoch 174/300\n",
            "198/198 [==============================] - 0s 168us/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 1.7997 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.57093\n",
            "Epoch 175/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.0363 - accuracy: 0.9949 - val_loss: 1.8312 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.57093\n",
            "Epoch 176/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 1.8505 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.57093\n",
            "Epoch 177/300\n",
            "198/198 [==============================] - 0s 165us/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 1.8686 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.57093\n",
            "Epoch 178/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0492 - accuracy: 0.9899 - val_loss: 1.8799 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.57093\n",
            "Epoch 179/300\n",
            "198/198 [==============================] - 0s 168us/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 1.8843 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.57093\n",
            "Epoch 180/300\n",
            "198/198 [==============================] - 0s 167us/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 1.8992 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.57093\n",
            "Epoch 181/300\n",
            "198/198 [==============================] - 0s 191us/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 1.8912 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.57093\n",
            "Epoch 182/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 1.8831 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.57093\n",
            "Epoch 183/300\n",
            "198/198 [==============================] - 0s 166us/step - loss: 0.0357 - accuracy: 0.9949 - val_loss: 1.8764 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.57093\n",
            "Epoch 184/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 1.8703 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.57093\n",
            "Epoch 185/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 1.8728 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.57093\n",
            "Epoch 186/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 1.8849 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.57093\n",
            "Epoch 187/300\n",
            "198/198 [==============================] - 0s 189us/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 1.9248 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.57093\n",
            "Epoch 188/300\n",
            "198/198 [==============================] - 0s 198us/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 1.9472 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.57093\n",
            "Epoch 189/300\n",
            "198/198 [==============================] - 0s 168us/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 1.9748 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.57093\n",
            "Epoch 190/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 2.0785 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.57093\n",
            "Epoch 191/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 2.2079 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.57093\n",
            "Epoch 192/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 2.2025 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.57093\n",
            "Epoch 193/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0310 - accuracy: 0.9949 - val_loss: 2.1977 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.57093\n",
            "Epoch 194/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.0399 - accuracy: 0.9949 - val_loss: 2.2674 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.57093\n",
            "Epoch 195/300\n",
            "198/198 [==============================] - 0s 196us/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 2.3302 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.57093\n",
            "Epoch 196/300\n",
            "198/198 [==============================] - 0s 214us/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 2.3537 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.57093\n",
            "Epoch 197/300\n",
            "198/198 [==============================] - 0s 195us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 2.3306 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.57093\n",
            "Epoch 198/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.0362 - accuracy: 0.9949 - val_loss: 2.2851 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.57093\n",
            "Epoch 199/300\n",
            "198/198 [==============================] - 0s 167us/step - loss: 0.0487 - accuracy: 0.9949 - val_loss: 2.2161 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.57093\n",
            "Epoch 200/300\n",
            "198/198 [==============================] - 0s 168us/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 2.1889 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.57093\n",
            "Epoch 201/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 2.2087 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.57093\n",
            "Epoch 202/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 2.1946 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.57093\n",
            "Epoch 203/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 2.1540 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.57093\n",
            "Epoch 204/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 2.0986 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.57093\n",
            "Epoch 205/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 2.0239 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.57093\n",
            "Epoch 206/300\n",
            "198/198 [==============================] - 0s 199us/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 1.9844 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.57093\n",
            "Epoch 207/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 1.9418 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.57093\n",
            "Epoch 208/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0302 - accuracy: 0.9949 - val_loss: 2.1461 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.57093\n",
            "Epoch 209/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 2.5214 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.57093\n",
            "Epoch 210/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 2.8101 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.57093\n",
            "Epoch 211/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.0394 - accuracy: 0.9949 - val_loss: 2.8364 - val_accuracy: 0.4267\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.57093\n",
            "Epoch 212/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 2.7843 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.57093\n",
            "Epoch 213/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0417 - accuracy: 0.9949 - val_loss: 2.5390 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.57093\n",
            "Epoch 214/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 2.2025 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.57093\n",
            "Epoch 215/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 2.2391 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.57093\n",
            "Epoch 216/300\n",
            "198/198 [==============================] - 0s 194us/step - loss: 0.0428 - accuracy: 0.9899 - val_loss: 2.3060 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.57093\n",
            "Epoch 217/300\n",
            "198/198 [==============================] - 0s 167us/step - loss: 0.0611 - accuracy: 0.9899 - val_loss: 2.3099 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.57093\n",
            "Epoch 218/300\n",
            "198/198 [==============================] - 0s 165us/step - loss: 0.0494 - accuracy: 0.9899 - val_loss: 2.2063 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.57093\n",
            "Epoch 219/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0500 - accuracy: 0.9899 - val_loss: 2.0698 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.57093\n",
            "Epoch 220/300\n",
            "198/198 [==============================] - 0s 199us/step - loss: 0.0344 - accuracy: 0.9949 - val_loss: 2.0126 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.57093\n",
            "Epoch 221/300\n",
            "198/198 [==============================] - 0s 196us/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 1.9776 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.57093\n",
            "Epoch 222/300\n",
            "198/198 [==============================] - 0s 197us/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 1.9713 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.57093\n",
            "Epoch 223/300\n",
            "198/198 [==============================] - 0s 190us/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 2.0780 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.57093\n",
            "Epoch 224/300\n",
            "198/198 [==============================] - 0s 191us/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 2.1262 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.57093\n",
            "Epoch 225/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 2.1868 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.57093\n",
            "Epoch 226/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.0364 - accuracy: 0.9949 - val_loss: 2.2356 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.57093\n",
            "Epoch 227/300\n",
            "198/198 [==============================] - 0s 185us/step - loss: 0.0309 - accuracy: 0.9949 - val_loss: 2.2468 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.57093\n",
            "Epoch 228/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0308 - accuracy: 0.9949 - val_loss: 2.3401 - val_accuracy: 0.4133\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.57093\n",
            "Epoch 229/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 2.4063 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.57093\n",
            "Epoch 230/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 2.4207 - val_accuracy: 0.4133\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.57093\n",
            "Epoch 231/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 2.4590 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.57093\n",
            "Epoch 232/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0306 - accuracy: 0.9949 - val_loss: 2.4393 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.57093\n",
            "Epoch 233/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0387 - accuracy: 0.9899 - val_loss: 2.3618 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.57093\n",
            "Epoch 234/300\n",
            "198/198 [==============================] - 0s 183us/step - loss: 0.0328 - accuracy: 0.9949 - val_loss: 2.1544 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.57093\n",
            "Epoch 235/300\n",
            "198/198 [==============================] - 0s 166us/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 2.2800 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.57093\n",
            "Epoch 236/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 2.4328 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.57093\n",
            "Epoch 237/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 2.4376 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.57093\n",
            "Epoch 238/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 2.2978 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.57093\n",
            "Epoch 239/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 2.2979 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.57093\n",
            "Epoch 240/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 2.2729 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.57093\n",
            "Epoch 241/300\n",
            "198/198 [==============================] - 0s 177us/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 2.2123 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.57093\n",
            "Epoch 242/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 2.1738 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.57093\n",
            "Epoch 243/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 2.1440 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.57093\n",
            "Epoch 244/300\n",
            "198/198 [==============================] - 0s 196us/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 2.1090 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.57093\n",
            "Epoch 245/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 2.0670 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.57093\n",
            "Epoch 246/300\n",
            "198/198 [==============================] - 0s 167us/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 2.0318 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.57093\n",
            "Epoch 247/300\n",
            "198/198 [==============================] - 0s 202us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.9949 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.57093\n",
            "Epoch 248/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.9808 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.57093\n",
            "Epoch 249/300\n",
            "198/198 [==============================] - 0s 207us/step - loss: 0.0226 - accuracy: 0.9949 - val_loss: 1.9555 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.57093\n",
            "Epoch 250/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 1.9304 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.57093\n",
            "Epoch 251/300\n",
            "198/198 [==============================] - 0s 166us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.57093\n",
            "Epoch 252/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 1.8873 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.57093\n",
            "Epoch 253/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.8562 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.57093\n",
            "Epoch 254/300\n",
            "198/198 [==============================] - 0s 197us/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.8463 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.57093\n",
            "Epoch 255/300\n",
            "198/198 [==============================] - 0s 179us/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 1.8982 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.57093\n",
            "Epoch 256/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 2.1194 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.57093\n",
            "Epoch 257/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 2.3034 - val_accuracy: 0.4933\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.57093\n",
            "Epoch 258/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 2.4266 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.57093\n",
            "Epoch 259/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 2.4767 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.57093\n",
            "Epoch 260/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0292 - accuracy: 0.9949 - val_loss: 2.3802 - val_accuracy: 0.4800\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.57093\n",
            "Epoch 261/300\n",
            "198/198 [==============================] - 0s 175us/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 2.3687 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.57093\n",
            "Epoch 262/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 2.3083 - val_accuracy: 0.5067\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.57093\n",
            "Epoch 263/300\n",
            "198/198 [==============================] - 0s 168us/step - loss: 0.0219 - accuracy: 0.9949 - val_loss: 2.2428 - val_accuracy: 0.5200\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.57093\n",
            "Epoch 264/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 2.1864 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.57093\n",
            "Epoch 265/300\n",
            "198/198 [==============================] - 0s 180us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 2.1216 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.57093\n",
            "Epoch 266/300\n",
            "198/198 [==============================] - 0s 170us/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 2.0693 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.57093\n",
            "Epoch 267/300\n",
            "198/198 [==============================] - 0s 169us/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 2.0307 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.57093\n",
            "Epoch 268/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 1.9822 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.57093\n",
            "Epoch 269/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.9845 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.57093\n",
            "Epoch 270/300\n",
            "198/198 [==============================] - 0s 172us/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 2.0582 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.57093\n",
            "Epoch 271/300\n",
            "198/198 [==============================] - 0s 173us/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 2.1082 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.57093\n",
            "Epoch 272/300\n",
            "198/198 [==============================] - 0s 183us/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 2.1244 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.57093\n",
            "Epoch 273/300\n",
            "198/198 [==============================] - 0s 219us/step - loss: 0.0187 - accuracy: 0.9949 - val_loss: 2.1071 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.57093\n",
            "Epoch 274/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.0960 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.57093\n",
            "Epoch 275/300\n",
            "198/198 [==============================] - 0s 198us/step - loss: 0.0200 - accuracy: 0.9899 - val_loss: 2.0904 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.57093\n",
            "Epoch 276/300\n",
            "198/198 [==============================] - 0s 190us/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 2.0691 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.57093\n",
            "Epoch 277/300\n",
            "198/198 [==============================] - 0s 174us/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 2.0562 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.57093\n",
            "Epoch 278/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 2.0332 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.57093\n",
            "Epoch 279/300\n",
            "198/198 [==============================] - 0s 190us/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 2.0021 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.57093\n",
            "Epoch 280/300\n",
            "198/198 [==============================] - 0s 171us/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 1.9621 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.57093\n",
            "Epoch 281/300\n",
            "198/198 [==============================] - 0s 197us/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 1.9315 - val_accuracy: 0.6267\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.57093\n",
            "Epoch 282/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.9095 - val_accuracy: 0.6267\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.57093\n",
            "Epoch 283/300\n",
            "198/198 [==============================] - 0s 183us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.9153 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.57093\n",
            "Epoch 284/300\n",
            "198/198 [==============================] - 0s 176us/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.9368 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.57093\n",
            "Epoch 285/300\n",
            "198/198 [==============================] - 0s 185us/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.9513 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.57093\n",
            "Epoch 286/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.9368 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.57093\n",
            "Epoch 287/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0210 - accuracy: 0.9949 - val_loss: 1.9249 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.57093\n",
            "Epoch 288/300\n",
            "198/198 [==============================] - 0s 181us/step - loss: 0.0193 - accuracy: 0.9949 - val_loss: 1.9234 - val_accuracy: 0.5600\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.57093\n",
            "Epoch 289/300\n",
            "198/198 [==============================] - 0s 178us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.9207 - val_accuracy: 0.5733\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.57093\n",
            "Epoch 290/300\n",
            "198/198 [==============================] - 0s 221us/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.9293 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.57093\n",
            "Epoch 291/300\n",
            "198/198 [==============================] - 0s 201us/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 1.9339 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.57093\n",
            "Epoch 292/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 1.9337 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.57093\n",
            "Epoch 293/300\n",
            "198/198 [==============================] - 0s 184us/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.57093\n",
            "Epoch 294/300\n",
            "198/198 [==============================] - 0s 188us/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 0.5867\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.57093\n",
            "Epoch 295/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.8932 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.57093\n",
            "Epoch 296/300\n",
            "198/198 [==============================] - 0s 186us/step - loss: 0.0204 - accuracy: 0.9949 - val_loss: 1.8837 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.57093\n",
            "Epoch 297/300\n",
            "198/198 [==============================] - 0s 191us/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.8885 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.57093\n",
            "Epoch 298/300\n",
            "198/198 [==============================] - 0s 223us/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.8757 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.57093\n",
            "Epoch 299/300\n",
            "198/198 [==============================] - 0s 182us/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.8194 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.57093\n",
            "Epoch 300/300\n",
            "198/198 [==============================] - 0s 213us/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.8221 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.57093\n",
            "75/75 [==============================] - 0s 133us/step\n",
            "[INFO] accuracy: 61.33%\n",
            "[INFO] Loss: 1.8220572980244953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6orODFV2WEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7c4e359f-4db8-4f64-f809-3de7881d57ab"
      },
      "source": [
        "plt.figure(1)  \n",
        "   \n",
        "# summarize history for accuracy  \n",
        "   \n",
        "plt.subplot(211)  \n",
        "plt.plot(history.history['accuracy'])  \n",
        "plt.plot(history.history['val_accuracy'])  \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'validation'], loc='upper left')  \n",
        "   \n",
        "# summarize history for loss  \n",
        "   \n",
        "plt.subplot(212)  \n",
        "plt.plot(history.history['loss'])  \n",
        "plt.plot(history.history['val_loss'])  \n",
        "plt.title('model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'validation'], loc='upper left')  \n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeXhU1fn4P+9MJvueQCBhCSBL2GQHRZTFBUVFLIpbLValtVq0P7W1q9hqa7+11lp36l43RBFUcMOouCEE2UHWAAkkkH3PzGTO749zs2+TkCHb+TzPPHPnnnvPfe/cmfOe877veY8opTAYDAZD98XW3gIYDAaDoX0xisBgMBi6OUYRGAwGQzfHKAKDwWDo5hhFYDAYDN0cowgMBoOhm2MUgaFbISIviMj9Xh6bKiLn+lomg6G9MYrAYDAYujlGERgMnRAR8WtvGQxdB6MIDB0OyyRzt4hsFZFiEXlWROJEZI2IFIrIJyISVeP4S0Vkh4jkichnIpJUo2ysiGyyznsDCKxzrYtFZLN17tciMtpLGeeIyPciUiAiR0RkSZ3ys6z68qzyhdb+IBH5p4gcEpF8EfnS2jddRNIa+B7OtbaXiMhyEfmfiBQAC0Vkkoh8Y13jmIg8JiL+Nc4fISIfi0iOiGSKyO9EpJeIlIhITI3jxonICRFxeHPvhq6HUQSGjsqPgPOAIcAlwBrgd0AP9O92MYCIDAFeA+6wylYD74qIv9UovgO8DEQDb1r1Yp07FngO+BkQAzwNrBKRAC/kKwauByKBOcAtInKZVW9/S97/WDKNATZb5z0EjAfOtGT6NeDx8juZCyy3rvkKUAH8CogFzgBmAb+wZAgDPgE+AOKB04C1SqkM4DPgyhr1/hh4XSnl8lIOQxfDKAJDR+U/SqlMpVQ6sA5Yr5T6XilVBqwAxlrHLQDeV0p9bDVkDwFB6IZ2CuAAHlFKuZRSy4ENNa6xCHhaKbVeKVWhlHoRKLfOaxKl1GdKqW1KKY9SaitaGZ1jFV8DfKKUes26brZSarOI2ICfArcrpdKta36tlCr38jv5Rin1jnXNUqVUilLqW6WUWymVilZklTJcDGQopf6plCpTShUqpdZbZS8C1wGIiB24Gq0sDd0UowgMHZXMGtulDXwOtbbjgUOVBUopD3AESLDK0lXtzIqHamz3B+60TCt5IpIH9LXOaxIRmSwiyZZJJR/4ObpnjlXH/gZOi0Wbphoq84YjdWQYIiLviUiGZS76qxcyAKwEhovIAPSoK18p9V0rZTJ0AYwiMHR2jqIbdABERNCNYDpwDEiw9lXSr8b2EeABpVRkjVewUuo1L677KrAK6KuUigCeAiqvcwQY1MA5WUBZI2XFQHCN+7CjzUo1qZsq+ElgNzBYKRWONp3VlGFgQ4Jbo6pl6FHBjzGjgW6PUQSGzs4yYI6IzLKcnXeizTtfA98AbmCxiDhE5HJgUo1zlwI/t3r3IiIhlhM4zIvrhgE5SqkyEZmENgdV8gpwrohcKSJ+IhIjImOs0cpzwMMiEi8idhE5w/JJ7AECres7gD8AzfkqwoACoEhEhgG31Ch7D+gtIneISICIhInI5BrlLwELgUsxiqDbYxSBoVOjlPoB3bP9D7rHfQlwiVLKqZRyApejG7wctD/h7RrnbgRuBh4DcoF91rHe8AvgzyJSCPwJrZAq6z0MXIRWSjloR/HpVvFdwDa0ryIH+DtgU0rlW3X+Fz2aKQZqRRE1wF1oBVSIVmpv1JChEG32uQTIAPYCM2qUf4V2Um9SStU0lxm6IWIWpjEYuici8inwqlLqv+0ti6F9MYrAYOiGiMhE4GO0j6OwveUxtC/GNGQwdDNE5EX0HIM7jBIwgBkRGAwGQ7fHjAgMBoOhm9PpElfFxsaqxMTE9hbDYDAYOhUpKSlZSqm6c1OATqgIEhMT2bhxY3uLYTAYDJ0KEWk0TNhnpiEReU5EjovI9kbKRUQeFZF9orNMjvOVLAaDwWBoHF/6CF4AZjdRfiEw2HotQk+XNxgMBsMpxmemIaXUFyKS2MQhc4GXrIRg34pIpIj0Vkod85VM3Y0Sp5t1e7M4LykOm02aPd5d4eH9bceYmBhNfGQQAMXlbj7YnkFRuZvJA6PZm1nEGYNi2J6eT2xoAKnZxZwo1Mkzx/ePYnSfyKr6PB7F3cu3sm7vCYb1DueGqYm8v/UYYYF+RAX7879vD5EYE8L5I+LYkJpDbrGLc4f3JKl3OKlZxTyevB+PUsSGBjCoZyjfHcwmMsifS07vzdH8MpJ3HyfY387Fo+PJL3Wx/mAO5yb1JCEyiNkjexEZrFPzf7M/mwfX7OLeS0cwrl8UR3JK+MUrmxjcM5QfMgsZ2y+SuLBA3t16lIJSN9MGx3L37KH0DAus/yUBX+/LAuD5r1MZGhdGel4puSVObjprIBMSo/jdim18uTeLiYnR/N/80ew4WsAzXxxga1peVR29IwJJiAoiu8jJxMRolqek4VGK6BB/Lh7dm/S8Ural53NeUi/Cg/xIzSpmQ2ouw3qHsf5ADoEOG1dM6Mvr3x2mxFkBgMNuY8rAGFIO5VTtawibCNMGx5IYG8L7W4+RVVSOCExMjGZUQgRrtmdwNK+00fOHxIUhAj9kFDIgNoTQAD+KnW5CAxxsTctjWO9wHlkwhugQ/f3vPFrA0nUH+Hp/FjWDFCOCHFxyejxhgX4czCrmk52ZuD2KYH8788f3YfbI3qz4Po2Vm4/idHubqbv6HicNiGbv8SKyi5pP7joyIYLicjcHs4oZ2isMpWBPZtORtb0jAomPDCK3xMn4/lG8lZKOx4sozJAAPy4e3ZtSZwVrtmdgtwkTE6P5LjWbcpe+T4fdxuSB0Xx/OI/icnfVub+ZPYwfje/T7DVaik/DRy1F8J5SamQDZe8BDyqlvrQ+rwV+Y037r3vsIvSogX79+o0/dKi2qcvlcpGWlkZZWVmb30Nnwu3x4HIrgvztAOSVOCkqryDY305IgB+uCg9BDjulrgoCHXb8bIJSigqlKHVWUO72UOqq4Ei+m33F/kw5rSd/fncn2cXOWtfpFx3M4ZySBmWYPCCaCYlRrN11HKfbw4GsYs4fHsf3R/KqFAagG57+0WQUlHE4p4SoYAeRwf4czCoGwCZwet9IhvUKY1t6PvuPF3P+iDgOZZew+Uge/n42zhsex4nCcr47mIOfTUjqHc629HwAghx2pp4Wy/qD2RSVu1EKYkL8CfCzoYDcEicVHsXQXmHsySjCWeFhXL9I+kQF8+GODIbHh3P1xH6EBfrx77V76REWwLh+UWQVlfPK+sP17jss0I/CMjd+NsHtUcwe0YuPdmZgE/05KtjBrKQ4HHatkFMO5XIsr4wKpShxVjBtcCx9ooLYeayQLdb9De4Zyo6jBQBV97frWAEzh/Vk7/EiDmYVMyQulPH99Ro9JwqdJP9wnDMHxdAnKqjR30lxeQUf7cygzOVhRHw4o/tEUO7y8PHOTArL3QzqEcLExGikgb6DxwOf7zmBQnHOkB58tS+bcncFwf5+FJa5mDG0J+9tO4bHo7BbnY9yt4dgfzvnD4+r+m0C7MksIuVQLgAOu3BuUhyRwQ6O5JTypaVsRWDm0J70DPdmiYj69zi4ZxgjE8KbPNZdoUj+4TiBDjtnDorh8z0nEIRzhvTA1oTNZGNqLhn59Z9hcxzOKeGrfdnYBGYOi8Pt8bBubxZnD46lV4TufGQXOfl093EmDYimf0xVLkLmjklgysCYxqpuEhFJUUpNaKisUziLlVLPAM8ATJgwoZ7mSktLIywsjMTERKShX28nRCmF2/oz2Rq5p8pjbCI43RUczCpBeTwMjAtDAXszi4hx2Chz6d5hAKBECFAKRAgLdpBX4kIpRSAQLMKQMH8Sc3ORXUe4/fXNJMYE89SPx9M7IpCPdmRSUObikU/2MiI+nCsn9KV/TDBj+kbidHtYufkoz391kPUHcxiVEEGZq4LLxybwzytPx1nh4d0txwhy2Lnv3R2UuSp45vrxhAc6KChzERLgh59NyC1x8bfVu9hxtIAXfzqJ8MD6i2bll7oI8LMR6NCNSkGZC4fNRpC/ncIyF4eyS3juq4N89sMJzk2KIzEmhNF9I7hr2RYG9Qxle3o+/7pyDBeO6g1AmUsrwYggfa13txzll699z/eHdQ8+MthBTrGTdXt14zR3TDyTBkSTGBPCE5/tIz4iiL9cNpJVW46y/0QRUwbGMGNoT77el8Xne0/QPzqEeWMTajWClew6VsDWtDyunNAXEa2YC8rcVfdXUObC41EE+NlrnZ9T7OTtTWksmNiXsAa+o+Yoc1VQ7vIQHuRX9Z9xuj2UON1EBDlO6n90/ZmJfLA9A2UlS+0RGsAV4/sSEVxfzsbub9exAtbtPcF5w3sxIDak1bKcCnYeLWBbevUz9IbCMhd2mxDs3zGa4PYcETwNfFaZ8ldEfgCmN2camjBhgqobNbRr1y6GDRvW6ZVAQamL/FIXoYF+FJa6ySt1Euzvx2k9Qykqc5Nb4iTY305MqO4dZRaUkVlQhoggYPU+PcSEBpBfohebGhwXisejKHZWoFBkFTqJCw8gp9hJUbmb0AA/wgIdRAQ58PfT3R+lFDt37WJ7cSizkuKIDa3dG/t4Zyan94mgZ3jDppPm2JNZSLnLw6g+Ea3/snzMB9uP0SMskEPZxYzrF0ViB2+MDIbm6KgjglXAbSLyOjAZvThGq/0DnV0JuCo8HM4pQSnd41WA3SaUON24KjxkFJRR4nSTWwKBDjvB/nZyi7WiCHTYcLo99IkK5kCWtonabcKgHqE47DawQ4DVe44J0Y16eJCDojI3oYF+9UYcInoUsmBiv7piAnDe8LiTutchcd5keW5fZo/Uo4VKs4vB0JXxmSIQkdeA6UCstSj3vehlA1FKPYVeW/YidOrfEuAGX8nSUSl3VyCAv5+dzIIyFNAvOohDlv29d2Qwabkl5JU4KXG66REWQG6Ji/S8UiKDHTgrPPSKCKxyigKEBviR43YSFx5YZTppCJsI4UFmrXKDweDbqKGrmylXwK2+uv6pJC8vj1dffZVf/OIXzR57LL8UmwiBDjsXz5nDg/9ZSmJ8T3JLXEQFO4gI9ie0WDsyo4IdZOTbyCzQTtbIYH9C/P1Iyy0hI78Mu03q2dAre/yVERsGg8HQHB3DU9HJycvL44knnqinCNxuN35+1V9xhUfVipx57rW3CHDYyS7W+ypt8f1jQgCFiBAe6EdOiZMAPzuBfjaCHHaGBYRT7q7Az26rFxYa5G+nj38wBoPB4C1GEbQB99xzD/v372fMmDE4HA4CAwOJiopi9+7d7Nmzh8suu4wjR45QUlrKFT9ZxI033UxYoB/jRgzluw0byM08zo1X/4gZ50zj66+/JiEhgZUrVxIUFERCVBA9wwOw22xVfhCbTQjqINEGBoOh89PlWpP73t3BTiv2uq0YHh/OvZeMaLT8wQcfZPv27WzevJmVaz7imvnz2L59O4mJiWQXlfP400tJiOvBwYxczjtnKrf99DqiQ2IBbavvFx1M6oF93LrsdZYuXcqVV17JW2+9xXXXXYeI4O/XuK3fYDAYTpYupwjak1JnBVlFTk4fN4H+iYmk5eoZp0sfeZgvPl6N0+0h81g6B/bvo2eP2KrzRIQBAwYwZswYAMaPH09qamo73YXBYOhudDlF0FTP3dfkleoZuI6AIA5nl1BQ5mJnyjd8+UUyKz9MptBt42dXXdrgDOiAgOpYfbvdTmlp41P8DQaDoS0xC9O0AWFhYRQWFpJX4sImoFAUlLmIjwwiUJUTFRlFgdvGgX172LTxu/YW12AwGGrR5UYE7UFMTAyTJp/BJdMnEx4aQlhULJHB/sSE+HPhhRfyxJNPcdmMyQw8bTBTpkxpb3ENBoOhFp1uzeLGUkwkJSW1k0Q6udvxwnI8SjE0Lgy3R+Fnk1qznU8UlhPksBMa2Dl0b3t/pwaDoW3pqCkmugTlroqqTJwJkUGISFWGyZr0CGtZ9kSDwWA4VRhFcJIUWrnCh8SFNZnSwWAwGDoqxll8khSVufH3sxHgZ75Kg8HQOTGt10ngrvBQbKVy7uzZTw0GQ/fFKIJW4vEoDpwoxgNEBZsEbwaDofNiFEEryS1xUuauoH90MCEBxtViMBg6L0YRtBClFDnFTrKKdDhoWCvCQUNDQwE4evQo8+fPb/CY6dOnUzdMti6PPPIIJSXVawdfdNFF5OXlNXGGwWAw1McoghZS6qogLbeEcreHHmEBJ+UbiI+PZ/ny5a0+v64iWL16NZGRka2uz2AwdE+MImghxeV6IfhhvcKqVga75557ePzxx6uOWbJkCffffz+zZs1i3LhxjBo1ipUrV9arKzU1lZEj9XLOpaWlXHXVVSQlJTFv3rxauYZuueUWJkyYwIgRI7j33nsBePTRRzl69CgzZsxgxowZACQmJpKVpRdYf/jhhxk5ciQjR47kkUceqbpeUlISN998MyNGjOD88883OY0MBkMXnEew5h7I2NamVXp6jSTjjCX0CAuguNxNgJ+tVmroBQsWcMcdd3DrrXrBtWXLlvHhhx+yePFiwsPDycrKYsqUKVx66aWNjiCefPJJgoOD2bVrF1u3bmXcuHFVZQ888ADR0dFUVFQwa9Ystm7dyuLFi3n44YdJTk4mNja2Vl0pKSk8//zzrF+/HqUUkydP5pxzziEqKoq9e/fy2muv1Ut3bTAYui9mROAFOr10ObnFToqdbkLqLAozduxYjh8/ztGjR9myZQtRUVH06tWL3/3ud4wePZpzzz2X9PR0MjMzG73GF198UdUgjx49mtGjR1eVLVu2jHHjxjF27Fh27NjBzp07m5T3yy+/ZN68eYSEhBAaGsrll1/OunXrAEy6a4PBUA+vRgQi8jbwLLBGKeXxrUgnyYUPtkk1SilyS1zkljgpcVaAUpwoKqfCoxqMErriiitYvnw5GRkZLFiwgFdeeYUTJ06QkpKCw+EgMTGxwfTTzXHw4EEeeughNmzYQFRUFAsXLmxVPZWYdNcGg6Eu3o4IngCuAfaKyIMiMtSHMnUIisu1U7jCo4gIdBATEkCFR+FvtxER5Kh3/IIFC3j99ddZvnw5V1xxBfn5+fTs2ROHw0FycjKHDh1q8npnn302r776KgDbt29n69atABQUFBASEkJERASZmZmsWbOm6pzK9Nd1mTZtGu+88w4lJSUUFxezYsUKpk2bdjJfh8Fg6MJ4NSJQSn0CfCIiEcDV1vYRYCnwP6WUy4cytgslTp1DaGCPEPxsNspdFeSUOOkVEVhvwXiAESNGUFhYSEJCAr179+baa6/lkksuYdSoUUyYMIFhw4Y1eb1bbrmFG264gaSkJJKSkhg/fjwAp59+OmPHjmXYsGH07duXqVOnVp2zaNEiZs+eTXx8PMnJyVX7x40bx8KFC5k0aRIAN910E2PHjjVmIIPB0CBep6EWkRjgOuDHwFHgFeAsYJRSarqvBKzLqUpDfSi7mFJXBcN6hVft83hUg0qgK2LSUBsMXYuTTkMtIiuAocDLwCVKqWNW0Rsi0vSsp05KqbOCIP/a2US7ixIwGAzdC2/DRx9VSiU3VNCYhunMuCs8OCs8RPubHEIGg6Hr462zeLiIVE1ZFZEoEfmFj2RqFW250lqZS08aC+qm6wt0tlXrDAbDyeGtIrhZKVWVxEYplQvc7BuRWk5gYCDZ2dlt1oCVuXWEbHdcaEYpRXZ2NoGBge0tisFgOEV4axqyi4goq6UVETvQYewmffr0IS0tjRMnTrRJfXnW3AFHYVCb1NfZCAwMpE+fPu0thsHQuSnJgW3LISQWRl7e3tI0ibeK4AO0Y/hp6/PPrH0dAofDwYABA9qsvuv+u57CMhcrbxvX/MEGg8HQEJ/9Db57Rm/3HA49mw4hr0duqlYmNYnoC6E92kS8mnirCH6DbvxvsT5/DPy3zaVpZ0qcbpZtOMKuYwWcM6Ttv2yDwdBNKC+Cza9B/6lw6CvY/2nLFMHWN+Htm+rvn/MwTLyx7eS08HZCmQd40np1Wd7bcowl7+o8PoN6hrazNAaDodOybRk4C+HcJfDOLVoRnNFMfM2hr2HlrTD2Ovjs71qJnLm49jFxw30irrfzCAYDfwOGA1VeRKXUQJ9I1U5sPFQ9DOsXHdyOkhgASP0Kdr8H5/0Z7PXTehgMp5Ttb0HOQTj7rqaPUwo2PAu9RkGfiTBoFqQ8Dy9cDCE9dI9+25tw3l+g+AR89AcoL4SMrVCWD2v/DJH94cqXISTmlNyat6ah54F7gX8BM4Ab6IKZSzceyiWpdzixof6cOejUPABDIygFH/xGpxSvcMKcf7a3RL4lNxU2vwoRfWDc9fXLS3Jgw38hIAxGXQF7P4JRV4K962WS75BUuOCD30LRcf39R/WvLsvap0cAygPRgyAqETK3wyX/BhH9PE/sggo37H4fdrytzwvtBduXQ9EJiBsBfafAGbfC9/+Daf/vlCkB8F4RBCml1lqRQ4eAJSKSAvzJh7KdUnKKnRw4UcyvZw/lF9NPa29xui/lRVCSrf9wGdt0z2jDszDrTxAY0d7StR2uUt24h/SAogz4+E+w01q8qPfp+lWTr/8DXz6stz97EMryAIExVzd9nfRN4CyCfmeenNLIT4PjuyF+7CltoNqM7P26Ny/oBjeghabf3e9BkZVGPuV5bfIBKMyAFy+BwqPoyhUERkJwrFYYAL1Gwk/e1dvblusev80PPn8QbA64fiUkVucQY+A5rb7N1uLtL6NcRGzo7KO3AelAlzKipxzKBWBC/+h2lqSboJR+2Wz63VkEbie8fBlk7YWYQRAQDuf/BZZdD0c3t8sfxCeU5MD/fqQbp8HnaQWgPLrnuPVNHWky+0HwC9QmMXc5bHoJhl0MPYbBuocgIAI2LNVhiX4BDV8nay8s1avXMeoKuHyp7qG2hleuhOM7ILwPLEqG0J6tq+dUUOECd41U7Ue/h5cvB4+VGzN+LCxcDf7NmH8r5yWV5lrmmn7QcwRsehmm/1aXv36tVso//1KXLfsx7PkQfrIK/EPq1zlqvn5tfxuW36BHujWVQDvhrSK4HQgGFgN/QZuHfuIrodqDlEO5OOzC6D5dqNfZUalwwWtXQ2kOXL9K94Y3PmsVio67ztwOV74EiVb67KObuoYiSP6b7glWsr3GmtVT79Dvm17S5oGgaN2TPJAMJVkw4acwaKa2Me9cpU1nD/aHK56HoRfWv9a+T/T7+Bt0L7ZnEky7s+Uy56dpJTD6Kq201vxGX7Mjcnw3vHixtr3XJHoQzH0csvfBql9qp+z85xpXjG4nvHqFHrn5BULeEVj4HrhKYM8a/T3s+wTSN8KC/2l/AOjfbHEWhMU1LefIyyHxrA6jUJtVBNbksQVKqbuAIrR/oMuRciiHEfER3XI28SlDKUj+q7an5qYCAm8uhINf6AZu0Ez9h4oaADn79WfQNtf0TbB7NWx5DYbPhW8eg+AYHWGx+VW44oWGe2AdjdQvIXqg7gmu/bPurV67HOz+ehQ084/QIwk8bvj2SXjhIu1IHDpHfx8iEB4P4xeCza6Vxps36HP9Q+Cif1SblfathZjBcPG/dB1r/6JHFMPmtEzm/Z/q97PuAFUBh7/1/tzdq7Wym/s4OHw8QdPthNevBkQ7YisbebHr30xEAvQ/QyvVT5ZA38kw5ef161EK3v8VHPiMKnPP3Ceg3xTwWH6A936lR7Ez/gBJl1Sfa7M3rwQq6SBKALxQBEqpChE5qzWVi8hs4N+AHfivUurBOuULgX+gTU0AjymlTvn8hHJ3BVvS8rl+Sv/mDza0nvVPwxf/B/3OgDN/qf+4H/5Wl53/QO3QuJrOuPhxujHa+zG4S2HXKt2YHt9V3es98Fn9Bq4kB776t25Uz/l163wMm16GvpOgRxutxXRiNwy7SDfqfoFaMQw+r7o8tGd1mOGgmbDunxAUpSOnavZeHYEw6WZtLlr7ZygvgLQNeqR186f6nNQvtblJBOY+BjkH4K2b4caPtN26IVJe0L6ZUVdCv8l6396PISxeK5Eew3TES3mhdlw3x7qHID1F29KHzYEpv2i9eao5jqzX93jFCzBiXuPHTb1DO223vNqwIvj2ST0iO/tuiB2iI3nGXqvLbDa48O/6e+o1uvkIok6Ct6ah70VkFfAmUFy5Uyn1dmMnWCOJx4HzgDRgg4isUkrVXXD3DaXUbS0Tu23ZmpaP0+1hQmJUe4rR+TjyndWzr0Rg4PTaMx+z9upeb2WY3NA5eihd6RsoytTO4abio4fP1Y1a7GD9B9/xjh6Cp30HX/xDN1w/rNGNX78zdMO/9yP9hz70te7FBsfoSIyWUJgJq27TppVLHqldlrlDm27Ce3tfX3GW7o32sCYW9T9Tvxqj18jmTTDhvWGeNb3n2FZ47gJttz7n11ppnjZLlzmC4KpXtc/gtau1nT8ktnZdG/4L798J9gDY8jpc8Fct765VcMZtugGvlD1rDySMb/6eC45VH//hVzoYYOrtcHwnJHgxc7/CDfs+1tunnde0w3v/Wu2EHTSr6TpFYMgF8On9OmKn5u9131r46PdawU7/nf6d1mXwebWVdxfAW0UQCGQDM2vsU0CjigCYBOxTSh0AEJHXgblA0yuvn2Kcbg/3vbuDiCAHUwZ2wmiI9qIwA56/qNoBV0n0QLj6DYg5TYfMPXu+HkKDNvtc/kz1n0sEzruv+WuNuEy/KjnjVv0+fK5+vXIlfP+yfk3/LRSka5MJApc9CZtfgY3P6wbI1gLT3wEr83ruQXCW6B68zQZpKfDfmXrCzw2r9TFup26EKu/N7QRUbUfuid36va1GF3XpPRrmPaWd66sW64iU/jUckeG94apX9HN748dw2RMgNv3yD4EPfw+nnQsXP6Kf27vWZKa+U7TJCrSfAbQtvjlFUJiho2ku+BtMuQVW/Ez7RzK3wa53Yd4zcPqCpuv48LfVaRom3gzn/KZ2w12So0cnAHs/gT6TIDC8fj11GTRLK4IDn8HoK2pc7/d6FDDv6YaVQBfF25nFrfELJABHanxOAyY3cNyPRORsYA/wK6XUkboHiMgiYBFAv379WiFK43ywI4Pt6QU8ce04IoM7TF5DWbMAACAASURBVB69js+ml7QSWLgawnrpfVl74Y3r4PGJ2smbdwj8Q7WTLSBcR134YmLYwHNg74e6/s/+pveduVj3YsPitBnlzYWQuk6PWLyl0jaecxCemKIV2RUvwFs/1fvTrDWZlIKX5+lIlcowwaemQniCvvdKqhSBD1d+Gz4XEiZoJ2bitPphkgnjtb3+rRvh0THV+8P7aPnPXQKRfeGXG3VDDjqEt7InHtlfjxgq76Up0jdZ1xynlf6sP2mz0q53td1+1S/182jMpr7xea0EJlvmm/VP6VHLnIdg4k06fv+JKbU7I5UKqzl6j9GjxI3P6k6G3aGd4id2wfn3tzy8tJPj7czi59EjgFoopX56ktd/F3hNKVUuIj8DXqT2qKPyOs8Az4BeqvIkr1mL/ceL9G80qeM4bjo8JTk6tn/QzNqhbzGD4Oa1sOs97QvwC9SKIn6sb+WZcKPuqfafCj+s1g3VkNnVPbpKp3PaBu8VQYW7WhHkHap+f3meNof1OwMOf6Mby/x0OPSlPuZflomrNFfbqz9Zos0jcx/T5iT/MO3s9SUTb9KKYFC9v5Jm1HwI611t1svYqhvZvlOqo1/8Q/TzrIvdT5voju+qX7b+aW0GnPeU/py2QY82KuuM6ANDL9Ix+Zc+qiN3Nr0E59xdv67MHbD6Lj1CueCvel/iNN1wr74bvnlcK1rQE7dsDj0i89YRbrNpv9Q7P4d/DtXOelepLmvOtNQF8dY0VKNbQyAwD71ucVOkA31rfO5DtVMYAKVUdo2P/wX+z0t52ozDOSXERwQR4GeihbxCKXjrJm3Xn/67+uWVk6GiEnWURh8v7MgniyOwutFryEkYGKGjZ9K/r1/mLIHkB3RoZs2Gb++H2q8xdA788L7e13+qHlWE9YaZf4AX5uiGb9e7euRz4d+1wxJ0g//t4/Dlv/RnvwB93KAZvnOWVjLyR5B3uOEZypUkTq1W4uoabcpryl9Rkz4T9cSoClftEd53SyF7r+7Bx42ErctgwNm1o7nOXaKjdcZcq+tIeR7OvK1+RFHl3Ip5T1eb85Iu1vWt+6dWJqnr9L2OX+id3HUZc7X2Hx1ZrxX63o/0/p7db61ub01Db9X8LCKvAV82c9oGYLCIDEArgKuAa+rU07vG+seXAg10M3zLoezirptXqKxAh+4FhGvn2L612nRwMg1R9n7tlDt3CfSd2PhxlVEWHYWEcTpMtSZKwcpfwI4V4Cyu7RD+bqnucU65RSsCuz9cs0zH0A+criOZxK6jlra/BWOu0aGsY6/T51e4deRJeb7uBW96Ue+f2EBGybbGzx+m/8b740V0BJK3DJqpG/C0jTocE7Tiyd6rtz9ZohvTgjS48MHa58YO1i/Q4agvzdX+jEsf1SOwQTO143/vx/o7ruvQDgzXfqWJN+qRwbSTjNqpfGYeD6z5te7A+FpRd0BaO+d8MNCkLUUp5bZmIX+IDh99Tim1Q0T+DGxUSq0CFovIpYAbyAEWtlKeVnM4p4Rzk7yM++1MVLjgjWvrN36LPjs5U83+tfp9+GVNH9fRiB8HW9/QZprKSJ9DX2klALVj47//n3YUz7q3utGKG6ntxpc9Xn1cnwnaZg3aPFUTux8Mv1SbXxb8T88iLTime7SdnQFnayW4f61WBK4yPZcDdEO+/1P9/UUPhCENTHSrZOB0OPvX2oxo94fN/9OT1ra+rsvPaUKZRfaDa95oqzvSpqI5D7VdfZ0Mb30EhdT2EWSg1yhoEqXUamB1nX1/qrH9W+C3Xknaxng8ioPZxWQVOenbFUcEa36jlcClj+mh7/cv6/3Z+09SEXyq/+DRbbcQ0CmhjzV6SXkeZlgmrQ3/1XlhzrhVm4fy0yH/CLx7h26kzlyszRKVYal1mfs4LJ2lwzwbisuf+5gedYjocFePp2v0NoMitRLct1bH5D83W0cCRfSD69628iChzWXN5Tcav1Args2v6M9bazTug8/3ifiG+nhrGvJi5kjnYuWWdH71xhYA+sd0MUWwY4V2qk29Hcb9GE6/Gsb9BJ49V4dCNkXRCR2LftmT1ROKKqlwwcF1zSc664gkjNO9zc//Xm23r3DqyKKhF2lF8O/R4KnQiu6KF6obsZs/1cnh6hI7GG77rvFcP1C74e9K4YiDZukIrRU/0+kn5vyzeuZzUAvm40Qk6LkJVVFISvs2JvzU90EGhiq8HRHMAz5VSuVbnyOB6Uqpd3wpnC/Znl5Qtd0/uhOkJvCG9E1aAWRs187RWffq/XY/bc8P7aVDIZti74c6vcO2N+srguM7wVXccO+4oyOio0viRugcR6AjTSYt0nboC/+hY95tfjD2x7Ubs+gmlt2oDJ3tbgyaCZ/9VTttJ958cr6PQTO1Iug9Bo5t1hPHjBI4pXjrI7hXKbWi8oNSKk9E7gU6rSI4UVgOwMIzExnWu5MPePIO6+yc25bpyBSA2X+vP3kqekDziqAyZLLSF1CTmnHhnRFHIExd3HDZ5EWnVpbOTsI4HY1Vlt8yR3NDjF+oQ5LP+pWOCDqt+4VvtjfeKoKGxrSdekWMI7klnDkohiWXjmhvUU6Okhx48VJt8vG3JsH0HAGnX1X/2OiB1Q29UjpWO866//RNUJYL+5PBEaxj4HMO1vYFHN2kbepRncw/YGh7bHYdAlp47ORnSvcYCpc/rbd/tPTkZTO0GG8b840i8jA6dxDArUCKb0Q6NRzJKWXmsC6wQH3yA3pGpD1Ap3K49D+Nx49HDdB/XGeJzt+y7HqdfybngM4DVMmM3+t6938K0VY0TIVLK4v4sV3D4Wk4eWb/rb0lMLQR3nqvfgk4gTeA14EytDLolJS5KsgqKqdvVCd3EpcX6uRgo+ZXr4Y0cEbjx8dYtu6Dn+s4eYAPfwcf/VGn0r3xY1j0uY7NjuhbPXooyYGHhug1ArxJNGYwGDoV3kYNFQP3+FiWU0ZabglA5w8b3famHgVMvEn39pMu0XliGmPwBdBzOCy/UTt9I/vpOPdeo/UMzpozQAfN1NFHFS4dV1+ao3Ovt3YWp8Fg6LB4NSIQkY+tSKHKz1Ei8qHvxPItR3J0TpG+0T5eKMPXHF6vI4ESxut1ZIfObvr4gFA9CSfpYhi9QCdIO/OXcPXr9Rd1GTRT57hf8TP49gm95u05d9fO/GgwGLoE3voIYpVSeZUflFK5ItJps7QdzNJLKvTr7GGjJ3brqfwtsdlH9tOpoCs5//6Gjxs0Q2fJPPC5DqlsaS5/g8HQafBWEXhEpJ9S6jCAiCTSQDbSzsL2o/n0CAugR1gTE4FagrNYR9o01yA7i/XiJJWEJzQ/87IxPB692Mc4Hy0dHRgBt7ZgSUKDwdBp8bYV+j3wpYh8jl7EcxrW+gCdkR3pBYyM92LxCm84sgFevERnnhzfRKNc4YYnp9ae2Xv61dUpe1tK/mG9kHbPYa0732AwGCy8dRZ/ICIT0I3/9+iJZKW+FMxXlDor2Hu8kAtGtCLRXNEJ3eiX5em0Amcuhs8e1EsCbn9LO1ULjsL0e3Rqh/JCvbD4oBk6q2HuQT1pJmawnvz1wxqd0qAlq2ZVcuIH/d7DKAKDwXByeJti4ibgdvSaApuBKcA3NLCITEdn57ECPApGJrRmIfMX9ApGY67TC3K///903vlBs6pn4sYMhndv17nNcw/q9XT3faxXgAqL15E3dj+tSA58pvPZ95nQ8PX2fKRj/BtaYLtyYRBfLXtoMBi6Dd7OI7gdmAgcUkrNAMYCeU2f0jHZnaFzDA1vqWmowg0bX9Bx+pc9Dte/o7MjXvlC9RT7aGuFriEX6B57hRvmPKwXz/APgZm/r/YJDJwBSHWsfkN88Q/44Dd6yb66HPlOLxvYkgRfBoPB0ADe+gjKlFJlIoKIBCildotIp+yKZhU6AYgLD2zZice26IU2KhdbD+sF176pt8uLdHbKqYu1k/Xq12qfO7FOrnrQ4Z59JuhY/bPvru9ornDpJQTt/nrJvtjBkHhWddnBL/REMoPBYDhJvB0RpFnzCN4BPhaRlcAh34nlO/JKnYQF+OGwtzAl8AnLFNNQVsSAULhzT8snW427Xmf0PPxN/bLju/Ri4rMf1DmC3l6kFQDotWCdhY2vSWswGAwtwKvWUCk1TymVp5RaAvwReBboZEtUafJKXESGOJo/sC4ndut8PlGJDZe3Jtf8yPkQEKFNQB5P7bKjVqbPgdPh3PugIF07lz9ZopPMia1rrHZlMBjanRYHsSulPveFIKeK3BInkUH+LT/xxA8QO6R1ET6N4R8Ms/6oTT9LZ+j1WCvJSdVmpuiB1kLwfeG9O/Si8UPnwIjL9EpRBoPBcJJ06lTSrSGvxEVkcCtGBMd311+opS2YeJMOR923ttr0A3rlpsELte9A7No38d1S6H06XPDXtlVIBoOhW9MNFYGTfi1NNldepCdw9WgkvfPJIKKdxWff3fRxI3+kXwaDwdDGdKFFVL0jtzUjgpz9+j12SNsLZDAYDO1Mt1IEFR5FQZmLyOAW+ggKjur3iD5tL5TBYDC0M91KERSUulAKIoNaOCIoPKbfw3q3vVAGg8HQznQrRZBboieTRbU0fLTgmA7XDOm0mbcNBoOhUbqVIsgr1VE5LTYNFR7TSqC1KaMNBoOhA9O9FIE1ImiVaSjcmIUMBkPXpFspgtxiPSKIavGIIMP4BwwGQ5elWymCrKJyAKJCWmEaCuvlA4kMBoOh/elWimDXsQJ6hQcS0RLTkLtcp3UIi/edYAaDwdCOdCtFsC09n5EJLVyHoDBDv5sRgcFg6KJ0G0VQVO7mQFZxy1cmy0/T7+FmRGAwGLom3UYR7DxagFIwqqWKoHKx+egBbS+UwWAwdAC6jSLYnp4PtEIR5BzU2T8j+vpAKoPBYGh/uo0imJAYxd0XDKVnS5eozDkAkX3B3orU1QaDwdAJ6DZTZUf3iWR0H2shl6LjekH44GiY8NOmc/vnHtSLwxgMBkMXpdsoglp88Q/47hm9nZsK5/0FjnwLfoF6TeKaC8nnHISR49pFTIPBYDgVdB9F4HZChRNcpbD5NRi9AAIj4ZvH4Mh3kPadPu6Cv8K4n+jtsny9ephxFBsMhi6MTxWBiMwG/g3Ygf8qpR6sUx4AvASMB7KBBUqpVJ8Is/5J+PhP1Z8n3AgJ4yHrBzjwGUxaBPnp8OHv9Ksm0YN8IpLBYDB0BHymCETEDjwOnAekARtEZJVSameNw24EcpVSp4nIVcDfgQU+Eaj/WdoEBBAaB30naRPQlS/D/rWQdCm4y2DL6+Asrj7PEQSnzfKJSAaDwdAR8OWIYBKwTyl1AEBEXgfmAjUVwVxgibW9HHhMREQppdpcmj7j9asugeEwYp7e9g+BiTe2+aUNBoOhI+PL8NEE4EiNz2nWvgaPUUq5gXwgpm5FIrJIRDaKyMYTJ074SFyDwWDonnSKeQRKqWeUUhOUUhN69OjR3uIYDAZDl8KXpqF0oOZ03D7WvoaOSRMRPyAC7TRulJSUlCwROdRKmWKBrFae29Ew99IxMffSMTH3Av0bK/ClItgADBaRAegG/yrgmjrHrAJ+AnwDzAc+bc4/oJRq9ZBARDYqpSa09vyOhLmXjom5l46JuZem8ZkiUEq5ReQ24EN0+OhzSqkdIvJnYKNSahXwLPCyiOwDctDKwmAwGAynEJ/OI1BKrQZW19n3pxrbZcAVvpTBYDAYDE3TKZzFbcgz7S1AG2LupWNi7qVjYu6lCcQXIfsGQ1dERF4A0pRSf/Di2FTgJqXUJydTj8FwKuhuIwKDwWAw1MEoAoPBYOjmdBtFICKzReQHEdknIve0tzwtRURSRWSbiGwWkY3WvmgR+VhE9lrvUe0tZ0OIyHMiclxEttfY16DsonnUek5bRaRFOcCt7+lu69xiEXlWROJEZI2IFIrIJzW/JxG5VER2iEieiHwmIkk1ysaKyCbrvDeAQODiynsRkYut51EmIuUistv6fFGNOn5r3csPInJBIzLfbB2TIyKrRCS+xnfxL+t6BdbzH2mVXSQiOy3Z0kXkrhZ+T31FJNmqY4eI3G7t98lz8SVN3MsS67vZ3Jrn0h6ISKCIfCciW6x7uc/aP0BE1lsyvyEi/tb+AOvzPqs8sVUXVkp1+Rc6fHU/MBDwB7YAw9tbrhbeQyoQW2ff/wH3WNv3AH9vbzkbkf1sYBywvTnZgYuANYAAU4D1rfievgXi0ClMjgObgLHohvxT4F7r2CFAMToxogP4NbDP+o34A4eAX1ll8wEXOlvuOOu448Bk4D7gNevaATXk+Kn1WwsABli/QTvwAnC/ddxM9OSgcdZx/wG+sMouAFKASOv7SAJ6W2XHgGnWdhQwroXfU+/Kc4AwYA8w3FfPxce/r8buZQlwVwPHD2/oubT3fViyCRBqbTuA9db3vQy4ytr/FHCLtf0L4Clr+yrgjdZct7uMCKoS4CmlnEBlArzOzlzgRWv7ReCydpSlUZRSX6DnidSkMdnnAi8pzbdApIj0buEl/6OUylRKpQPr0I3W90qHK69AKwXQmW7fV0p9rJRyAQ8BQcCZ6D+fA3hEKeVSSi1HT5I8bN1LFPC0Umo9oNANdrl1XiVnAq8rpcqVUgfRymNSHVmvRc+x2aSUKgd+C5xh9exc6IZtGDqwY5dS6ph1ngsYLiLhSqlcpdSmlnxBSqljlecopQqBXWjF6cvn4hOauJfGmEvzz6VdsL7fIuujw3opdIdhubW/7nOpfF7LgVkiNVfW8o7uogi8SYDX0VHARyKSIiKLrH1xNRqGDHQvuLPQmOxt8awya2yXNvA51NqOR/f6AVBKeaxrJ1hl6crqalnUTG3iD9wpInnonvP/AYOB+2qYnmK9uJe6MhSh06wkKKU+BR5Dp3M/LiLPiEi4deiP0L30QyLyuYic0ch30SyW0hmL7n368rn4nDr3AnCbZcp6rsZz6dD3IiJ2EdmMHnF+jB6x5CmdmBNqy+tV4s7m6C6KoCtwllJqHHAhcKuInF2z0GqwOmUscDvKfpQa+VesnlRfdEqUY0BCnd5VvxrbLuABpVSkVYcDPUHzK+CfJyFDCPqPnA6glHpUKTUebc4YAtxt7d+glJoL9ATeQZsOWoyIhAJvAXcopQpqlnW231QD9/IkMAgYg36eLXku7YZSqkIpNQadn20SekToU7qLIvAmAV6HxjJzoJQ6jjZvTAIyK4fn1vvx9pOwxTQm+6l8VsuAOSIyS0QcwJ1o887X6PxXbmCxiDhE5HJqmw9ygJ+LyGRL9kC0kn61xnFZXtzLa8ANIjJG9Ip9f0WbslJFZKKITLZkKwbKAI+I+IvItSISYZm0CgBPS2/eqvct4BWl1NvW7o7wXFpMQ/dimQcrrJHeUqqfS4e+l0qUUnlAMnAG2hRXmQmiprxV9yJeJu5siO6iCKoS4Fne9qvQCe86BSISIiJhldvA+cB2qpP2Yb2vbB8JW0Vjsq8CrreiVKYA+TVMFW2KUuoH4Dq0gzYLuAS4RCnltHxJlwML0Y3+AuDtGqeXATejTTd5aDvzQquOyuiob4CrrMiOAWjT0Xd1ZPgE+CO6ETuG7sFW5twKRzdguWjzUTbwD6vsx0CqiBQAP0f7GrzGGuk8C+xSSj1co6jdn0tLaexe6vgw5lH9XFbRzHNpL0Skh4hEWttB6ECGXWiFMN86rO5zqXxeXiXubBBfeb872gttT92Dtrf9vr3laaHsA9FRDluAHZXyo00Ia4G9wCdAdHvL2oj8r6EbORfavnljY7KjoyYet57TNmBCe8vvxb28bMm61fpj9q5x/O+te/kBuLC95a8h11los89WYLP1uqgzPpcm7qUzPpfRwPeWzNuBP1n7B6KV1T7gTaqj0wKtz/us8oGtua5JMWEwGAzdnO5iGjIYDAZDIxhFYDAYDN0cowgMBoOhm+PThWl8QWxsrEpMTGxvMQwGg6FTkZKSkqUaWeq30ymCxMRENm7c2N5iGAwGQ6dCRA41VmZMQwaDwdDN6TaK4HB2CR/uyMCEyxoMBkNtuo0iWLP9GD97OYUSZ0V7i2IwGAwdik7nI2gIl8tFWloaZWVljR4zPsLN0kt7c3DfHuy2Fmdp7VYEBgbSp08fHA5He4tiMBhOAV1CEaSlpREWFkZiYiKNpeLOK3FyOKeE0+LCCHTYT7GEnQelFNnZ2aSlpTFgwID2FsdgMJwCuoRpqKysjJiYmEaVAFA1CqjwGB9BU4gIMTExTY6uDAZD16JLKAKgSSUAYBejCLylFQscGQyGTkyXUQTNYbNGBB4TNWQwGAy16DaKwE85iZUCn4wI8vLyeOKJJ1p83kUXXUReXl6by2MwGAwtod0VgYgEish3IrJFRHaIyH2+uI6tvIB4yUbcpW1ed2OKwO12N3B0NatXryYyMrLN5TEYDIaW0BGihsqBmUqpImu5uS9FZI1S6tvWVHbfuzvYebSggRIFzmI8kovNEdSiOofHh3PvJSMaLb/nnnvYv38/Y8aMweFwEBgYSFRUFLt372bPnj1cdtllHDlyhLKyMm6//XYWLdJrz1emyygqKuLCCy/krLPO4uuvvyYhIYGVK1cSFNQyOQ0Gg6E1tPuIQGmKrI8O6+UDQ77gwg+bqgDV4uVdm+TBBx9k0KBBbN68mX/84x9s2rSJf//73+zZsweA5557jpSUFDZu3Mijjz5Kdnb9JUX37t3Lrbfeyo4dO4iMjOStt95qUxkNBoOhMTrCiAARsQMpwGnA40qp9XXKFwGLAPr169dkXU313Pdl5DLQcwhbcDRENl3PyTBp0qRaMfiPPvooK1asAODIkSPs3buXmJiYWucMGDCAMWPGADB+/HhSU1N9Jp/BYDDUpN1HBABKqQql1BigDzBJREbWKX9GKTVBKTWhR48Gs6h6h82PIls4lOSAu/zkhG6CkJCQqu3PPvuMTz75hG+++YYtW7YwduzYBmP0AwICqrbtdnuz/gWDwWBoKzqEIqhEKZUHJAOzfVG/TYQciQYRyE+DNgolDQsLo7CwsMGy/Px8oqKiCA4OZvfu3Xz7batcHwaDweAz2t00JCI9AJdSKk9EgoDzgL/74lp2m1BWYYewXlBwFEpzIDim+RObISYmhqlTpzJy5EiCgoKIi4urKps9ezZPPfUUSUlJDB06lClTppz09QyGDoO7HCpcEBDa3pIYToJ2VwRAb+BFy09gA5Yppd7zxYXsNtETykJ6QlkB5KWB3R8Cwk667ldffbXB/QEBAaxZs6bBsko/QGxsLNu3b6/af9ddd520PAaDz0lLgWXXg90BP/scAiPaWyJDK2l305BSaqtSaqxSarRSaqRS6s++upbdJnpCmQhEJYKfP2Tvh7zD4DE2eYOhRaxdAu4y/f/54LftLY3hJOgII4JThk30iMCjFDa7A2IHQ8ExKMnWI4So/m0yOjAYugU5B+G0WSA22PNBe0tjOAnafURwKvGrm4HU5geRfbVCEBtk79NO5PJCqHC2mTPZYOhyuJ1QkA6R/SFuhO5MleS0t1SGVtKtRgR+dq333BUeHPYaOtA/BHoM1Q7k4hP6BSB2CI6GsN5gM2sYGAxV5B/REzOjEiEkVu/L2gv9JrerWIbW0b0UgTUicHkU9ZI32Ox6dBDWS9s9XWXgLNZKwV0O0QO1b8FgMEDeIf0elaj/MwBZe4wi6KR0K0XgsOuG3F3RhMnH7tCvgDCgBxSHaHNRcRaEnsRkNoOhK5Gbqt+j+kNoLx19l723XUUytJ5u5iOoNg15TXAs+IdCUQZ42mbh+9BQHXN99OhR5s+f3+Ax06dPZ+PGjU3W88gjj1BSUlL12aS1Npwycg/pxj+sN9j9IHqQNg0ZOiXdShHYbILdJrhbsiaBCITH6/DSgvQ2lSc+Pp7ly5e3+vy6isCktTacMnJTdb6uSt9Z7GlGEXRiup5paM09kLGt0eIBTrdercyvBc7fXqNg6mIoytS9oNC4Wv6Ce+65h759+3LrrbcCsGTJEvz8/EhOTiY3NxeXy8X999/P3Llza1WbmprKxRdfzPbt2yktLeWGG25gy5YtDBs2jNLS6nUTbrnlFjZs2EBpaSnz58/nvvvu49FHH+Xo0aPMmDGD2NhYkpOTq9Jax8bG8vDDD/Pcc88BcNNNN3HHHXeQmppq0l0b2oaCdIjoU/05oh/sW6sj7U7Wl+apOLXBGfs+0ZNMe48+ddfsYHSrEQHo9XhbFRUa1gsCI6HwmHaKlWRrJ7JSLFiwgGXLllUdumzZMn7yk5+wYsUKNm3aRHJyMnfeeSeqiQs/+eSTBAcHs2vXLu677z5SUlKqyh544AE2btzI1q1b+fzzz9m6dSuLFy8mPj6e5ORkkpOTa9WVkpLC888/z/r16/n2229ZunQp33//PWDSXRvaiMIMbRaqJDweXCVQmnty9X79GDzQG96/S4eoAng8sGMFrH+67ZNFHlwHr1wBb93YrcPFu96I4MIHmyw+kV1CqcvN0F7hLa87KlHnJyrM0LMpAcTO2P4RHM/M4Gh6OieysoiKiqJXr1786le/4osvvsBms5Genk5mZia9evVqsOovvviCxYsXAzB69GhGj67unSxbtoxnnnkGt9vNsWPH2LlzZ63yunz55ZfMmzevKgvq5Zdfzrp167j00ktNumvDyePxWIqgxm85IkG/F6TrkOuWknsIVt0GB7+AniNgw1LomQQTb4T9a+HNhfq4gHAYc/VJ3wKgG/53b9ej/Kw9cGQ99OueucC63YjAzy64mooaagoRnaSu53CIHQoRfSEoEpzFXHHhOSx/7lHeePEZFsy/nFdeeYUTJ06QkpLC5s2biYuLazD9dHMcPHiQhx56iLVr17J161bmzJnTqnoqMemuDfXITdWh0t5SmgMeV50RgWUmym+lH+39OyH9e5j1J523qM9E+PJfelRw8HOwOSAgAg4kN1+XtxzfCTn79TX9Q2HFz2DnyrarvxPRLRWBR6mTW8ReO5Md5gAAIABJREFUBPyD9USayH4QN4IF1y7k9Xc/YfnK97hi+mjyMw/Ts0cPHA4HycnJHDp0qMkqzz777KrEddu3b2fr1q0AFBQUEBISQkREBJmZmbUS2DWW/nratGm88847lJSUUFxczIoVK5g2bVrr79fQNVEKPvoj/HsMPDYRUr/y7rzCY/q9wRFBmn6vcOuXN+z/FPZ9DNPvgWl36vDtMxfrSWtHvtVy9ZkIg8+D/cltZ8LZvVq/j/wRXL4U/ALh7Z/p1BndjG6nCAKsGcVOd9uEggIgNkZMOJPCUicJfQfQO3EI1845i43frmPU8GG89PyzDBs2rMkqbrn5RopyMkgaMog//f4exo8fD8Dpp5/O2LFjGTZsGNdccw1Tp06tOmfRokXMnj2bGTNm1Kpr3LhxLFy4kEmTJjF58mRuuukmxo4d23b3a6jGU9FmYcWnnD0fwNePwqj54AiC166GE3uaP68wQ7/XHBGExumULfnpsO6fcH8PeGKKd9/N+qf1+ZMWVe/ra01MO/QNHNsMiWfBoJlQfBxemqsdvCfLnjWQMEErtGEXwXVv63t471fdzl8gTTkwOyITJkxQdePrd+3aRVJSklfnl7kq2JNZSN+oYKJC/H0hosZZrB3Kpbl6Kr5fgDYrBUbq7UqU0sdVhqbaA8BdClEDtNmpnWjJd9ql8HjA5mX/qDATXrwYeo+BHy31rVxtjdupG2qbHW75Wvfyn5kOMYPhpx80Hfmz6SVY9Uu4Y1vtJV//NVI3qkc361n6OQfg+pUwcHrjdaVvgqUz4ey7YOYfapf94zTdMBceg+tX6ei9txfBid16tDD/Od2bbw2uUvhrApx1hzYNVfLdUlh9F8x7Bk5f0Lq6OygikqKUmtBQWbcbEfj72RARytpyRNDghUIss9FI7UsQP53L6PhOOLal+pWxTf+oHcHQYxjEDtHbeYf1gh81cRZDqZkw5jPK8uHR0yH5r94d//bN2sm4bRlk7vStbG1F2kbdy/7yX9o+fsFftSkmsh/M/KM2xWx7UzfQjd1T5YggNK72fkcwpG3QHZ0fv6Nt+lteb1yWdf+EpTP0f2XcT+qXx43USsA/DPqdoZ3Q1y2H2zZCwnhY/Wsozm7d95CxDVQFxI+rvX/CT7UZ6sPftr7uTki3UwQ2EQL8bJS7WjC7+KQuaNe+hB5DoEeSdqoFx+rRQXCM7vVH9oOY0/QfyGbTGR2VR0dSVLh0yFzeYd3o5B6sttEa2pZty/X3/Pnf/3975x0eV3E97Pds0a56t2RLLrINNsa9gA3GQDBgE8CmxBBKIEAgCfCDQAgQAoEvlTRCElNCb6GGbgIYA6bZxr33XiTLVi/bd74/5sqWZDXLK63Wmvd59tnd2+bMzt17Zs45cwbWzWr5WH8NbPtKPzjikuDrhw89JhzuWmajit3w1JnwzBT4/Pcw8Extd69j1JXQa5RWcE+cDo9O0D3wxnWoKtT3bv2RLegoH9A99fS+cPw0WPvewTDQ+mz5HOb8BoZMh5sW6hFEY3KtpcsHnqHXDqnD6Ybz/gG+SnjpovaFrO5eot/zGikCm11f21sBH99z+NeNUY4aRXA4Ji63w9bxI4KmcLp1vqLUPD0ZJzVfK4GEzIZDcadb7/NXwd5VehRRW6InvcRn6B6Zr7rDxIw1c2HEWPK8Dl1MyYPlL7d87J6lukd57BQYfomONvFWNjzm7R/DgwU6Nr4rsPEj3cE4/19w7t9hWiO57A64+gOYeBuc/Qf9vuJV+LiRyabxHII6vvtXuGkxHHu2/n7sVPBX67DM+vhr4d3/g8wBMP1RPQehKXKG6fdBUw/dlzsUZrwARavgqbO0Gepw2LNE50hqquycIXDyrfoe2Pxp0+eHw213hscAR4UicLvdlJSUtPkB5nLa8QfDRxY51NEkZukQ1ZRe2rSUfdxBBWKPs9IAR15+pRQlJSW43e6IX7tLU1monZIjLoWBk2HL3ENNc/XZ+a1+zx8HIy/Tfp01bx/c7ynXk6BUCGbfd3DeSTTZ8LHueIy6Asb+sGHUTx1xCTD51zDhp/r9hBtg/iMNH4glm/SotTGJWTrVRB0Fp2gbf2PH7ux7dfbSc/+uy2uOwd+F034JQ6Y1vX/QFPjB21BdDE9O1sq5rexZqkc/zTHpDp0/6f2facVVn/X/g4dHwIN94f3b9OixvWGzXYSoTigTkd7A80AOoIB/K6WaGGO3TH5+Prt27WLfvn1tOt4TCFFS7UeVuYhzxIourFe3gFdnQy2s1maJCON2u8nPz2/9wKOJurQk+eMgowCWPKcf9v1Obvr4XQv1gyIhA+LTtZN18XPavCKiTSIhP1z0JLxxDcx/FKb8ofPq05iAR8fjj7ri8FJAnPmAVgIf/AJuXKDNlCWb4PgLWj/XlQy9x2tFMPl+Xe6ad2HhkzDhJq0oWjw/CU67s+Vj+k2E6+bACxfAixdrR3fWMS2fE/C0XgenG857WAcDfPpbmGL5jYrX6fZML9Apt5f9BxY9pRXeqCu0r6OxuSkGiPbM4iBwu1JqiYgkA4tFZLZS6rA8b06nk4KCgjYfv7fSy4W/n8N95w7hmoltP6/LoJS29ZZth+tm6xnPhrazezG8djWc82fdqwQo0vM2yDlev9scOra9viLYt147SF3JsGMeDDpHbxeBE2/Q0SZ1s1PXvqvb5bjzYejFWkmc+gutNKLBulk6BcRx5x3eec54HVXz2pWw4jUdzKDCOoKnLQy9EGbdps8dcYmeMZxeAJMfOPw6NEfWQLjyLe37eOYc/bnOv9AU+zfoOvRoJSqu4BQYdx3Mn6lHTyfdrGcixyXClW/qbQGPNk8tfxmWvgiLn4UR34dz/qIVWYwQ1e6wUqpQKbXE+lwFrAXyOrrcnBQ3PZJdrNxd0dFFdQwi2qEV8ush8cf36pvR0DqeMnj5MqjYAR/cof/IoEcE6f3AnaJfdT3ZOj66B2aeAC9M10rAU6ZNSHWMvEw/5L/5p7Yf71wABZN0W510MwRqYOFTnVrVBix/RQcq9J14+OcOtkJk/3fnQd9JTgsP2vqMuVrPCfjg59oZv/VL7VOxR7gPmjVQ+zdsDnj2u9rfE/A0fWzxWv3eY0jr1536Jz1ymH2frv/O+XDa3QfNas546D0Ozv0b3LEJTvm59qs8PSWmoo66jF1ERPoBo4AFTey7XkQWiciitpp/WmN4fhordsVwKGbOELjqPR3+Nv8ReOxk+Otx8Pfh8NAwmHkifPJA+9eRDQW0k/CDO2DX4taPjxVWvqHXlph8v1YG8yyHadGKhr3cgWdo5VBVpEcQ8/6lH+x7luoHjdhgQL2JfHGJMO5Huue94UMddZJ/gt6XO1QrjXkzO39d33BYT9jaPAeGf6/tcyTqY7PBJS/qHu7CJ3S+n6Z8BE2ea9dRREk5OrEbCobPOHwZ2kL2sdo0lJqv5zk8erIOlW1M8RrtZ8vo3/o1bXbt0M4dBt8+rv12o65s+lh3CpxxL1z2ul6k54XpB0NtAfau1iPDspazDESDLjGhTESSgLnA75RSb7Z0bFMTytrDP+Zs5KFPNvDVnd8hLy3G0zDX7NeOyZ3f6h6o2HXK7C2fgTtVhziOu6756IymqJs05IjXjtDT79EOtFhfrvOJM/RSpD/5Gl65XKcsuPYjeGyidkzW2aSLVupt02bCho+0ff1nq3WY6Bd/1qOHW5Y3vHZ1sZ5UFbIyZN74rV4LG/SI7fFJ2o58/j8iW6egv2F4ZX15Xr5UK7JjztJpFI5kkuK+9bqnmztUd0IOB0+Z9pvYXR0/UUsprfje/xmU79QjkNPu0r4f0NlGK/foe6CtBH3apJTRXyv91tj4Cbz2A33shBthx3w9k7mOniMguZdWNPs36jkdNqceWZ5yW8MU3xGipQllEVUEInIL8AxQBTyJ7uHfpZT6uIVznMD7wEdKqb+1VkakFMGWfdVM+9fX5Ka6ee/mibidR+Hi9EWr4PM/6F6qzQHDvqd7KoUrdPTIxJ/pYW9cgo4VL1qp31Pz4anJer7DVe/qhGArXtUhdZPvjw1lsGM+fP5HHcqYXgC7F+mQ2wWPwpm/0etLlG7VIydnPHjL4acLoIeVCkQpHRnicOk/6im3696eUtoW3GMI5I85tNzZv4av/64/31fWsAc+63bdI7xj45H5CkJB/aDb+oWelLjtKx3O2XO47rn2GAK+Kj1SLN8B5z6kH4aRaLeK3frh1VTEUVfDVw1z/3jQJDf1T9pH8s/ROl3FRU92bPl7V8M7N+lQVVcKnHyLvh83fqw7IN5yrcQzB2rTVvU+PZlPhSAxW/urQgFtWjv5Fj3iOAI6UxEsV0qNEJGzgRuAe4EXlFJNutFFRIDngFKl1K1tKSNSigDg3eV7+L+Xl/LK9eMZ3z8zItfskpRu1b3Yla9D7nAdGbNzgb5Bxaadnwrw1fOZ2JxwxX+h/6navPDB7bDoaeh7slYIfcYf8Y3ZYexcCM+eo4f//kbzLYZerHvkdb26z36vJ5AdczZc/lrDY+fNhI9+qZOR3bqqbWtWh0Pwzo16JDb1wYb7di/RE7XO/xeMvlKbLZa9qPPdjL6qdbNNOKRt/V/+RcfNO9y6hzrwDN37L1xhOUKtOTIp+TB9ZsspHroDFbvhnZ/qSWzJPfVvdc1H2rbf0YTDegJeUk7b/CLlO2HZS1qBF63QHbjC5brDduGT2sndTv9KZyqCFUqp4SLyMPC5UuotEVmqlGoyYFdEJgJfAiuBuqm+v1RKfdBcGZFUBCXVPsb89hPunjqYG04dEJFrdmnqrx4VDsOWT2HHAm3PDgegz0k63cDuxToRWc8RDc/99gnd263Li5Q3RqclqG8rjzbhMDx5hv7z/eQbbZKoKtQx40odGskR8MD/fgEn/vhgxFAd3grtcxnx/VbXuWgTSsE/RulZtFe9B0+eqX9rFdIO2Yue0mGLdfXYvQgqdunZvyoM/71O9yZzh2sz3bFnHzq7N+CBks16lJNe0D6fwNFIKAjz/qkzjg6fASf8KNoStZ25f4LPfqc7bec9DKN/0K7LdKYieAYd9VMAjADsaIXQxBi6fURSEQBM+tNnDM1L4ZHLIybi0U3QD+s/0OaSZS/qXPaTH9DJu7oCy1+Ft67XDr6Rlx359WpL9YjJ7jzyawF8+TeY8wCc9TudwuCs32lTy4d361HWuOt0mYuf0U5N0Ll24tOhag9M+aM+JhbMc4bIEAroeSjeCp0lNa99z6rOVAQ2YCSwRSlVLiIZQL5SakWkyoi0Irj55aUs3lbKN3efEbFrdhsCXr2Yx9p34epZ0Pek6MlSWQjr3tfJ1BKz4Uefdc3ecMCrc/iUbtGO09vX6Ulpy1/RysBjRRX1OB5Oukn7cpa+qFONnP2H1idhGQzN0JIiiPSEsgnAMqVUjYhcAYwGDnumcGcysnca7y3fw66yWvLTW5jubjgUp1vnqylaoc0WP/6q4TKFQb+Ou944WycIyx0GQy6AxAj5Y7Z9rcM6wyHtMA3U6OHzRU92TSUA+jeb8YIOMR1w+sHfa8SlcPyF2sbvjNd5eOro147Yf4PhMIi4jwBtEhoOPIuOHJqhlDo1UmVEekSws7SW0//yOVeM78v95x/f+gmGQ9mzTE9sS+qh7enuFB3KuuVz7ay1ObXztHa/dn7lDtfHItp+L3Jw/Ya+J2vn55Bph0bW+Kq1uUQp2L9ez3FwpegRQN4oGH+jTuCXnNOUlAZDt6YzTUNLlFKjReQ+YLdS6qm6bZEqI9KKAODON1bw1rLdzLvrO2QmuVo/wXAo27+BT3+nZ92qkI5YOcZKc1xwqo7S2btKRy4VrdShcqDDEMMBPfchqYeeeVq5SyuOaTN1uJ+3Ql978bMHY/RB56i/5EWd7MxgMLRIZ5qGqkTkbuBK4BTLZxAhL1vHMWNcPq8u2sni7WWcdXwMxEd3RfqeBD+cpaNWwkGdDK+xQzN3WOs5apTSs3dn3Q6vXqlz+Kx6U48mRl6uM1LarNt2wBld1wRkMMQQkVYElwCXAdcopYpEpA/w5wiXEXGG9EzFJrBqT6VRBEeK8whnaYvo7I1Xz4IXL4IFj2lz0dmvQ6+RkZHRYDA0IKKKwHr4vwSME5FzgW+VUs9HsoyOID7OzsAeSayK1SR0RyNxCTqLZFXhwdQABoOhQ4jouFpEZgDfAt8DZgALROTiSJbRUQztlRq72UiPVpxuowQMhk4g0qahe4BxSqliABHJBj4B3ohwORFnaF4qby7dTVGFl9zUbrY6l8Fg6NZE2tNmq1MCFiUdUEaHcNJAHdv+0eqiVo40GAyGo4tIP6Q/FJGPRORqEbkamAU0mzeoKzE4N4Vheam8unBntEUxGAyGTiWiikApdQfwb/SEsuHoNYhbWXS06zBjbD5rCitZsCV2VhYyGAyGIyXiZhul1H+VUrdZr7ciff2O5KIx+eSlxfPrd1cTDIVbP8FgMBiOAiKiCESkSkQqm3hViUhlJMroDBLiHNx33hDWFVXx7Dfboi2OwWAwdAoRUQRKqWSlVEoTr2SlVBddvaRpzhqSw+mDsnlo9gaKKrzRFsdgMBg6nJiI6OlMRIQHzh9KMKz4zaw10RbHYDAYOhyjCJqgT2YCN54+kFkrClm+szza4hgMBkOHYhRBM1w1oR82gTnrils/2GAwGGKYqCsCEXlaRIpFZFW0ZalPaoKTYflpfLVxX7RFMRgMhg4l6ooAvYDNlGgL0RSnDMxi+a4K1hXFTOCTwWAwHDZRVwRKqS+A0mjL0RSnD+5BKKyY8vcv+WTN3miLYzAYDB1C1BVBWxCR60VkkYgs2rev80w1Y/qm879bTiEvLZ6nvtraaeUaDAZDZxITikAp9W+l1Fil1Njs7OxOLfu4nilcMb4v87aUMPXhL3nww3V4A6FOlcFgMBg6kkinoT4q+f4JvVm8vZQaX4hHP9+M027jtjOPjbZYBoPBEBFiYkQQbdIS4njyqnG8fP14Jh/Xgxfnb6fGF4y2WAaDwRARoq4IRORlYB4wSER2ici10ZapJa6fNIDSGj+jfzOb5+dt44JHvuabTfujLZbBYDC0m6grAqXU95VSPZVSTqVUvlLqqWjL1BInFGTwn+tOZFBuMve9s5qlO8p5ccH2aItlMBgM7SbqiiAWOWlgFjMvG83g3GQG5ybz2bp9ePzGgWwwGGITowjaSe+MBD68dRK/+u4QPIEQs9eaeQYGgyE2MYrgCBnfP4Njc5J44N3VXPvsQj5fb3ITGQyG2MIogiPEYbfxyOWj8QfDfLlpPzf/ZykzHp/HBysLoy2awWAwtAmjCCLAwB7JLPzVZObcdioup52Vuyq49+1VVHgC0RbNYDAYWsUoggjhdtrpnZHAwnvO4LUbJlBa6+efczYCEA4rlFJRltBgMBiaxswsjjAiwrD8VGaM6c2z32zDbhNeW7STUFhx6+RjuWZiQbRFNBgMhgaYEUEH8fOzB5EQZ+fxL7YwPD+NYfmp/GbWGuZuMOsbGAyGroXEmsli7NixatGiRdEWo02U1fhx2IVktxOPP8S0mV9RVhtg9s8mER9nRyltUjIYDIaORkQWK6XGNrXPmIY6kPTEuAOf4+Ps/G3GSKbP/JoLH/mGkho/Vd4AU4f25PIT+7CuqIohvVIY3z8zihIbDIbuiFEEncjQvFQeu2IMj87dTP/sJPpmJvDsN9uYZYWaZiTG8eUvTmd/tY9ASDGwRxIASilCYYXDbix5BoMh8hjTUJRZX1RFYYWHUFhx7XOLOH1QNvO2lOBy2Jl92ySUghtfWkK1L8h7N0/Eabfx+fpiFm8v48bTBxrTksFgaBPGNNSFGZSbzKDcZAAmH5fDnHV7Gdc3g2W7yrnuuUUUVXgpq/UTCCneWrIbl9PGba8tJxRWfLlxP6/dMIE4x8GRQkVtgLkb93He8J6ISLSqZTAYYgijCLoQT/xgzAET0NtLd/Pgh+tIdjt47poTuOvNldz7zir8oTAnFmRwwag87vzvSu5/bzWjeqcxrl8G/bIS+cvH63lh/naKK71cd0r/aFfJYDDEAMY01IVRSh3o1e8qq+WxuZsJBBUPTDset9POjS8tOeBfcNqFl380nmufW0StP0hYwczLRjNlaC4Ae8o9PPP1VjyBEDdMGkDvjISo1ctgMHQ+LZmGjCKIYTz+EBuLq3A77fzwmYXsr/bhC4Z59PLRPP7FFpbtLKdPRgLTR+XxnwU7qPQEsNnAYbNxzcn9yEl1s3FvNb5gmOkjezEoN5laf4hP1xVzfK8URvZOM+Ylg+EowSiCbsCaPZU8OnczSS4Hv50+lFp/kNcX7eKtpbtZubuCoXkpPDRjJG6nnQfeW8MnVtrsxDg7IkJ1E0tv9slI4LRB2UwflUffjAQWbisDFAu2lvKdwT1YuK2MwbnJTB2ai4jgC4YorvQB0DPV3WyUU909Z5SMwdB5dGlFICJTgIcBO/CkUuqPLR1vFMHhEQor9lZ66ZnqbvDgLan24Q2G6ZnixhsM8eGqIsprA4SVYsKATNbsqWTWykK+3VpKbSuL7mQnu1AK9lf7DmzLSXFxfK9U4uPsjO2bTmKcg/V7q8hOdvHx6iJKa/xcO7GAUX3SeWPxLnaU1nLX1MHsr/YdKLNfZiIDshPpmRqPPxQiJ8VNsttJIBSmvDZAdrKrgRw+qx6biqu5ZFxvclPceINh/Nbry437iHPYyEx0MapPGomu5l1kZTV+Fm0vw+WwMapPGr5gmIVbS/EFwygUw/PTGJCdRLUviF2E+LiD0VvBUBi7TYyiM3QpuqwiEBE7sAE4E9gFLAS+r5Ra09w5RhF0LtW+IB+sKKTSG2BYXipxDhs5KW4e/mQjJw3MxBcM8+3WUpx2oWdqPLkpbkJK8cmavRRVeqn0BthZ6gHA7bThDYRJiLOTnx7Phr3VB8pxOWz4gmEARMBpt+G3vtcnyeVAgCpfkIkDs5h4TBbLdpQTDIfZsLeaHaW1baqX22ljXL8MNhVXU5CVSJzDRoUnwMjeacxZW9zgOg6bEAw3/J/YbcJJAzJZtK2M+Dg75wzLpawmwMbiKrbur6FfZiLfOa4H8zeX4LTbOH9kL1LjnWzZV0NGYhw2m+APhnE7bVR5gwzpmcKYvunsKK2lrMbPgB5JpMY7WVNYydId5ewsrWV4fion9s8kLy2eDXur+GrjfqYOy6VnajxKKSo9QYoqveyr8qFQxNltDM1Lxe20s2ZPJb0z4klLiCMcVniDIdYXacWclxYPwOZ9NczbvJ8hvVIZ0ze9yd9tf7UPbyBEXlo8IsK+Kh++YAiloMIToCArkdV7Knlr6W627q9maK9UspNdfHd4T9IS4iiv9ZOVpDsOTruwq8xDVrKLsFIEgmE8gRBV3iCZSXEUVXjpm5lIitvB5n01VHj89EqLJ9HlIMXtbCCXUooKT4DUeKdRwM3QlRXBBOB+pdTZ1ve7AZRSf2juHKMIYo/1RVWEworjeiazvaSWhDg72ckudpTWsmxnOT2S3eSnxzNvcwkp8U4mHZuF22GnsNLLxr1VFFf5cDlsFFV4Kazw4guGyU528drCnRRVeumdEU+K20my28ENkwZwTE4Sby7ZDUC8047Trh/kEwZk4nLY2VPuYc7avXy1aT8DspPYWebBYRPiHDYWb9fmrgtH5zEiP41QWPHFxv2kxDsY3z+T1HgnobDi9UU7+XKjPr+s1s+awkpS450c0yOJgqxE3l62h5JqH+P6ZVDhCbCuqOqwfzcRqPt71leUyS4HVZYpr05posAfOlRxOu2Cy2Gn2hdEBGwihBoptbrfqNJ70DyYnx6PUvoB63TY8AXChJViX7UPpbQMeenxbN5XTSCksAnUv2ycw0b/rES27q85IHd97DYhxe2grLb1VO3JbgdV3oamy9wUN5Ve68EP1PhDVHgC9MtMICfFTW6qmz3lHqp9IfLT48lOdhFnt+ENhCir9eOw20hPcJKeEEcwrPh4dRFKwbD8VMprAwRCYZJcDhJdDjIT4wiEwgTCWlkFQmFcDjs2G3gD+ri9lV76ZycRDIUpqw2Q5LLjdtoJhBTJbgdup51af5Aqb5CyWj9FFV6SXA4G90ympNpPMKzITnbhDYSo9ASp9AbwBkIM6ZWCTYQKT4CsJBdnDclhaF5qq79ZU3RlRXAxMEUpdZ31/UrgRKXUTc2dYxSBoY5AKExRhZf89PiI9QLLa/0kuRxHPIvb4w/hCYTIsNKMrCuqpNobZETvNCo8AYIhRZzDRq0/SLLbyYItJawrqqIgK5H0hDg2FVdRUuNnaF4qo3qnkZXkYl1RFQu2lrCxuJpjeyQxtl8Gc9YWUxvQD8nsJBc5KW6yk13YbUK1N8jCbaVUWeUWlnvwBcM47TacDqF/ViLFVT52lNTiD4U5JieZCf0zeX/FHnaU1CIiiIA/GMblsGG3CbmpbrKSXKwvqmJvpZfeGQmkJzjxBcPkp8ezt9LH0LwURvVOP5BiZev+Gr7YsI9qX5D0hDjKav3U+ILsKfdwQkEmVd4AdpvgtNtwO20kuhwUV/romepmy/4adpbWMqpPGj2S3eyp8FDpCbK2sJLMpDgqPAFsIsQ77eSmulmwtRSPP8iuMg+5qW4yE+PYVlJLeW0AXyCEO85OeoKTYEhRVuunwhNAASN7p5HidrJlfzVJLidupw2PP0SlJ0C5J6B/M7sNp13LWesPEQyHcdhsVPsC9EyNZ2dpLXabkJXkotoXxBsI4bAJNZZpVUSPaFPjnfRKjae01s+m4mqyklw4bMK+ah8JTjsp8bpT47ALawursAkku52U1/r544XDmTGud7vuyZhXBCJyPXA9QJ8+fcZs376902U1GAxHH+GwQqT9gQtKKcJKj3CaIxRW+IIh4p32Q8rxB8MHJoTWDxevIxAK47D8Tf6gHpW1N5tAS4og2slrdgP11Vu+ta0BSql/K6XGKqXGZmdnd5pwBoPh6MaEo8xSAAAGcklEQVR2hE59EWlRCYBWEglxjibLqZ8VoKn9TrvtwPY4h63DUspEWxEsBI4RkQIRiQMuBd6NskwGg8HQrYhqigmlVFBEbgI+QoePPq2UWh1NmQwGg6G7EfV5BIeLiOwD2uskyAL2R1CcaGLq0jUxdemamLpAX6VUk7b1mFMER4KILGrOWRJrmLp0TUxduiamLi0TbR+BwWAwGKKMUQQGg8HQzeluiuDf0RYggpi6dE1MXbompi4t0K18BAaDwWA4lO42IjAYDAZDI4wiMBgMhm5Ot1EEIjJFRNaLyCYRuSva8hwuIrJNRFaKyDIRWWRtyxCR2SKy0XpvOndwlBGRp0WkWERW1dvWpOyi+YfVTitEZHT0JD+UZupyv4jsttpmmYicU2/f3VZd1ovI2dGR+lBEpLeIfCYia0RktYjcYm2PuXZpoS6x2C5uEflWRJZbdXnA2l4gIgssmV+1MjEgIi7r+yZrf792FayUOupf6FnLm4H+QBywHBgSbbkOsw7bgKxG2/4E3GV9vgt4MNpyNiP7JGA0sKo12YFzgP8BAowHFkRb/jbU5X7g500cO8S611xAgXUP2qNdB0u2nsBo63Myel2QIbHYLi3UJRbbRYAk67MTWGD93q8Bl1rbHwN+Yn3+KfCY9flS4NX2lNtdRgQnAJuUUluUUn7gFWBalGWKBNOA56zPzwHToyhLsyilvgBKG21uTvZpwPNKMx9IE5GenSNp6zRTl+aYBryilPIppbYCm9D3YtRRShUqpZZYn6uAtUAeMdguLdSlObpyuyilVN2KTU7rpYDvAG9Y2xu3S117vQGcIe3IotddFEEesLPe9120fKN0RRTwsYgsttJyA+QopQqtz0VATnREaxfNyR6rbXWTZTJ5up6JLibqYpkTRqF7nzHdLo3qAjHYLiJiF5FlQDEwGz1iKVdK1a3OU1/eA3Wx9lcAmYdbZndRBEcDE5VSo4GpwI0iMqn+TqXHhjEZCxzLsls8CgwARgKFwF+jK07bEZEk4L/ArUqpyvr7Yq1dmqhLTLaLUiqklBqJTst/AjC4o8vsLoqgTesedGWUUrut92LgLfQNsrdueG69F0dPwsOmOdljrq2UUnutP28YeIKDZoYuXRcRcaIfnC8ppd60NsdkuzRVl1htlzqUUuXAZ8AEtCmuLlt0fXkP1MXanwqUHG5Z3UURxPS6ByKSKCLJdZ+Bs4BV6DpcZR12FfBOdCRsF83J/i7wAytKZTxQUc9U0SVpZCu/AN02oOtyqRXZUQAcA3zb2fI1hWVHfgpYq5T6W71dMdcuzdUlRtslW0TSrM/xwJlon8dnwMXWYY3bpa69LgY+tUZyh0e0veSd9UJHPWxA29vuibY8hyl7f3SUw3JgdZ38aFvgHGAj8AmQEW1Zm5H/ZfTQPIC2b17bnOzoqImZVjutBMZGW/421OUFS9YV1h+zZ73j77Hqsh6YGm3568k1EW32WQEss17nxGK7tFCXWGyX4cBSS+ZVwH3W9v5oZbUJeB1wWdvd1vdN1v7+7SnXpJgwGAyGbk53MQ0ZDAaDoRmMIjAYDIZujlEEBoPB0M0xisBgMBi6OUYRGAwGQzfHKAKDoRMRkdNE5P1oy2Ew1McoAoPBYOjmGEVgMDSBiFxh5YVfJiKPW4nAqkXkIStP/BwRybaOHSki863kZm/Vy+E/UEQ+sXLLLxGRAdblk0TkDRFZJyIvtSdbpMEQSYwiMBgaISLHAZcAJyud/CsEXA4kAouUUscDc4FfW6c8D9yplBqOnslat/0lYKZSagRwEnpGMujsmLei8+L3B07u8EoZDC3gaP0Qg6HbcQYwBlhoddbj0cnXwsCr1jEvAm+KSCqQppSaa21/Dnjdyg2Vp5R6C0Ap5QWwrvetUmqX9X0Z0A/4quOrZTA0jVEEBsOhCPCcUuruBhtF7m10XHvzs/jqfQ5h/oeGKGNMQwbDocwBLhaRHnBgHd++6P9LXQbIy4CvlFIVQJmInGJtvxKYq/RKWbtEZLp1DZeIJHRqLQyGNmJ6IgZDI5RSa0TkV+gV4WzoTKM3AjXACda+YrQfAXQa4MesB/0W4IfW9iuBx0Xk/1nX+F4nVsNgaDMm+6jB0EZEpFoplRRtOQyGSGNMQwaDwdDNMSMCg8Fg6OaYEYHBYDB0c4wiMBgMhm6OUQQGg8HQzTGKwGAwGLo5RhEYDAZDN+f/AxbKyeh8IJy5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGNrv37l2e01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate accuracy on test dataset\n",
        "# Define architecture\n",
        "model = Sequential()\n",
        "model.add(GlobalAveragePooling2D(input_shape=test_data['arr_0'].shape[1:]))\n",
        "#model.add(Dense(400, activation='relu')) \n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5, seed=seed_value))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbEsCG0A2oP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load saved weights\n",
        "model.load_weights(top_model_weights_path)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMN2Mr5r2o9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd3cfb89-6cb2-4c9a-fb72-ab584ca5c6fc"
      },
      "source": [
        "### Calculate classification accuracy on the test dataset.\n",
        "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_data['arr_0']]\n",
        "\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_labels, axis=1))/len(model_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 45.6140%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgp-VQu5IXVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52d611b7-697c-4a12-9b7d-48de35a6ab63"
      },
      "source": [
        "len(test_data['arr_0']), len(test_labels)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57, 57)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t39EDNKQ_FY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29cbd508-7d1d-4bbe-ddbf-c7a4ddf42d24"
      },
      "source": [
        "print(model_predictions), test_labels"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 9, 12, 1, 1, 1, 4, 2, 6, 6, 6, 6, 1, 12, 4, 6, 4, 4, 5, 6, 5, 6, 6, 6, 6, 6, 6, 8, 7, 6, 6, 0, 8, 16, 1, 9, 12, 10, 6, 2, 11, 11, 9, 11, 12, 12, 6, 12, 13, 1, 2, 2, 6, 16, 6, 4, 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1.]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLObPsTs41T2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate accuracy on train dataset\n",
        "# Define architecture\n",
        "model = Sequential()\n",
        "model.add(GlobalAveragePooling2D(input_shape=train_data['arr_0'].shape[1:]))\n",
        "#model.add(Dense(400, activation='relu')) \n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5, seed=seed_value))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJ6BZ_H41dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load saved weights\n",
        "model.load_weights(top_model_weights_path)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl5aP8qX41hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Calculate classification accuracy on the test dataset.\n",
        "model_predictions_train = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in train_data['arr_0']]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUe3Xz3S41k9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37386e22-453a-4a4f-f8e5-78105eb83a34"
      },
      "source": [
        "# report test accuracy\n",
        "train_accuracy = 100*np.sum(np.array(model_predictions_train)==np.argmax(train_labels, axis=1))/len(model_predictions_train)\n",
        "print('Train accuracy: %.4f%%' % train_accuracy)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 100.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF0P1ts041sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate accuracy on validation dataset\n",
        "# Define architecture\n",
        "model = Sequential()\n",
        "model.add(GlobalAveragePooling2D(input_shape=validation_data['arr_0'].shape[1:]))\n",
        "#model.add(Dense(400, activation='relu')) \n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5, seed=seed_value))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKxsGRBn41wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load saved weights\n",
        "model.load_weights(top_model_weights_path)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm2Btbwq411Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Calculate classification accuracy on the validation dataset.\n",
        "model_predictions_validation = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in validation_data['arr_0']]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af3Mxej-5Sb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50051793-412c-4fd8-c115-69c2fbb03f40"
      },
      "source": [
        "# report validation accuracy\n",
        "validation_accuracy = 100*np.sum(np.array(model_predictions_validation)==np.argmax(validation_labels, axis=1))/len(model_predictions_validation)\n",
        "print('Validation accuracy: %.4f%%' % validation_accuracy)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy: 58.6667%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHh2Nsa15ShG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3965b002-ec0e-4023-9fa6-5915c123b372"
      },
      "source": [
        "train_accuracy, validation_accuracy, test_accuracy"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100.0, 58.666666666666664, 45.6140350877193)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH9Q-lDK2rQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7f40d818-ad4d-4a1f-c4d5-855f4f98f7b5"
      },
      "source": [
        "# image_path = 'data/eval/Malabar_Pied_Hornbill.png'  \n",
        "image_path = '/content/faceImages/faceImages2/test/001.Ankit/Ankit_0008.jpg'\n",
        "   \n",
        "orig = cv2.imread(image_path)  \n",
        "   \n",
        "print(\"[INFO] loading and preprocessing image...\")  \n",
        "image = load_img(image_path, target_size=(224, 224))  \n",
        "image = img_to_array(image)  \n",
        "plt.imshow(image)   \n",
        "# important! otherwise the predictions will be '0'  \n",
        "image = image / 255 \n",
        "\n",
        "plt.imshow(image)   \n",
        "image = np.expand_dims(image, axis=0) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading and preprocessing image...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-dd7a7d2e81d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading and preprocessing image...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/faceImages/faceImages2/test/001.Ankit/Ankit_0008.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9GjbMh82vGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the VGG16 network  \n",
        "model = applications.VGG16(include_top=False, weights='imagenet')  \n",
        "   \n",
        "# get the bottleneck prediction from the pre-trained VGG16 model  \n",
        "bottleneck_prediction = model.predict(image) \n",
        "print(bottleneck_prediction.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GlobalAveragePooling2D(input_shape=bottleneck_prediction.shape[1:]))\n",
        "#model.add(Dense(400, activation='relu')) \n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5, seed=seed_value))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.load_weights(top_model_weights_path)  \n",
        "   \n",
        "# use the bottleneck prediction on the top model to get the final classification  \n",
        "class_predicted = model.predict_classes(bottleneck_prediction)  \n",
        "print(class_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e2C8E9827Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inID = class_predicted[0]  \n",
        "  \n",
        "class_dictionary = generator_top.class_indices  \n",
        "   \n",
        "inv_map = {v: k for k, v in class_dictionary.items()}  \n",
        "   \n",
        "label = inv_map[inID]  \n",
        "   \n",
        "# get the prediction label  \n",
        "print(\"Image ID: {}, Label: {}\".format(inID, label)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPkpExC43Cot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}